{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Adapted SLO TBL topic classification codebase and Derek Fisher's code for my own LDA topic extraction implemenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "#### Resources Used:\n",
    "\n",
    "https://pypi.org/project/lda/\n",
    "\n",
    "Implements LDA using collapsed Gibbs sampling algorithm.<br>\n",
    "\n",
    "https://www.tablesgenerator.com/markdown_tables\n",
    "\n",
    "Easy-to-use markdown table generator.<br>\n",
    "\n",
    "https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158\n",
    "\n",
    "Utilized two diagrams and explanation of associated notation in the section below.<br>\n",
    "\n",
    "https://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/\n",
    "\n",
    "Utilized blog's example as the basis for the explanation of the LDA alogrithm pseudocode.<br>\n",
    "\n",
    "https://www.coursera.org/learn/ml-clustering-and-retrieval\n",
    "\n",
    "Information on collapsed Gibbs sampling and variational inference.<br>\n",
    "\n",
    "https://www.investopedia.com/terms/p/posterior-probability.asp\n",
    "\n",
    "Explanation of statistical terminology including posterior and prior probability.<br>\n",
    "\n",
    "https://cs.calvin.edu/courses/cs/x95/videos/2018-2019/\n",
    "\n",
    "Used Derek Fisher's explanation of why LDA does not work well on Tweets (with Scikit-Learn standard implementation).<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "##### Basic Concept:\n",
    "\n",
    "Each document described by a distribution of topics.<br>\n",
    "Each topic described by a distribution of words.<br>\n",
    "\n",
    "Typically uses bag-of-words feature representation for documents.<br>\n",
    "\n",
    "Permits the identification of topics within documents and the mapping of documents to associated topics.<br>\n",
    "\n",
    "##### Terms:\n",
    "\n",
    "Observed layer: documents (composites) and words (parts).<br>\n",
    "Hidden (latent) layer: topics (categories).<br>\n",
    "\n",
    "k — Number of topics a document belongs to (a fixed number).<br>\n",
    "\n",
    "V — Size of the vocabulary.<br>\n",
    "\n",
    "M — Number of documents.<br>\n",
    "\n",
    "N — Number of words in each document.<br>\n",
    "\n",
    "w — A word in a document. This is represented as a one hot encoded vector of size V (i.e. V — vocabulary size).<br>\n",
    "\n",
    "w (bold w): represents a document (i.e. vector of “w”s) of N words.<br>\n",
    "\n",
    "D — Corpus, a collection of M documents.<br>\n",
    "\n",
    "z — A topic from a set of k topics. A topic is a distribution words. For example it might be, Animal = (0.3 Cats, 0.4 Dogs, 0 AI, 0.2 Loyal, 0.1 Evil).<br>\n",
    "\n",
    "![lda](lda-presentation-images/lda_model.jpeg)\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "##### Mathematical equivalent of the above plate diagram representation of LDA:\n",
    "\n",
    "![mathematical_model](lda-presentation-images/lda_equation.png)\n",
    "\n",
    "##### English Translation:\n",
    "\n",
    "Given a set of M documents with each containing N words and each word generated from a topic \"k\" from a set of K topics, find the joint posterior probability of:\n",
    "\n",
    "θ — A distribution of topics, one for each document,<br>\n",
    "z — A single topic from the N words for each document,<br>\n",
    "β — A distribution of words, one for each topic,<br>\n",
    "\n",
    "Given:\n",
    "\n",
    "D — All the data we have (i.e. the corpus),<br>\n",
    "\n",
    "Using the parameters:\n",
    "\n",
    "α — A parameter vector for each document (document — Topic distribution).<br>\n",
    "η — A parameter vector for each topic (topic — word distribution).<br>\n",
    "\n",
    "\n",
    "##### Joint posterior probability: \n",
    "\n",
    "In Bayesian statistics, it is the revised or updated probablity of an event occurring given new information.<br>\n",
    "Calculated by updating the prior probability using Bayes' Theorem.<br>\n",
    "In other words, conditional probability - probability of event A occurring given that event B has occurred.<br>\n",
    "\n",
    "##### Prior probability:\n",
    "\n",
    "In Bayesian statistics, it is the probablity of an event occurring before new information is given.<br>\n",
    "Calculated using Bayes' Theorem.\n",
    "\n",
    "##### Note:\n",
    "\n",
    "Choose # of topics < # of documents to reduce dimensionality for further analysis using other algorithms.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "α — Distribution related parameter that governs what the distribution of topics is for all the documents in the corpus looks like.<br>\n",
    "\n",
    "θ — Random matrix where θ(i,j) represents the probability of the i th document to containing the j th topic.<br>\n",
    "\n",
    "η — Distribution related parameter that governs what the distribution of words in each topic looks like.<br>\n",
    "\n",
    "β — A random matrix where β(i,j) represents the probability of i th topic containing the j th word.<br>\n",
    "\n",
    "##### Dirichlet Distribution (example with 3 topics):\n",
    "\n",
    "![dirichlet](lda-presentation-images/dirichlet_distribution.png)\n",
    "\n",
    "1) Large values of α pushes the distribution to the center.<br>\n",
    "2) Small vlues of α pushes the distribution to the edges.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simplified Latent Dirichlet Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode for the LDA algorithm:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Assign topic (z) to each word (w) in each document (d) (randomly or based on some probabilistic distribution)\n",
    "\n",
    "while(NOT exhausted time constraints)\n",
    "\n",
    "    for each document (d)\n",
    "        for each word (w)\n",
    "            for each topic(z)\n",
    "\n",
    "                Compute Probability(t | d)\n",
    "                Compute Probability(w | z)\n",
    "\n",
    "            Assign new topic (z') to w in d (based on computed probabilities).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "1) Assign a topic to each word in each document for every document based on some probabilistic distribution or randomly.<br>\n",
    "\n",
    "2) Repeat the next sequence of steps within each for loop until we have exhausted our allotted compute time.<br>\n",
    "\n",
    "    a) For each document; for each word within each document; for each topic within our set of topics;\n",
    "    \n",
    "        i) Compute the probability of that topic given that document.<br>\n",
    "        ii) Compute the probability of that word given that topic.<br>\n",
    "        \n",
    "    b) Assign a new topic to the word in that document based on the two computed conditional probabilities.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics for our example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Topics (k=2)      |\n",
    "|-------------------|\n",
    "| Topic 1 (food)    |\n",
    "| Topic 2 (animals) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "It should be noted that the topics in a LDA model are actually just indexed values (1-N) and not actually described by any sort of noun, verb, etc.<br>\n",
    "\n",
    "We later assign \"food\" and \"animals\" as the descriptors for the two topics as we see that the top N words for each indexed topic are strongly associated with those descriptors.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial topic assignment for each word in each document:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    Documents (M = 5, N = 3)    |   Word 1   |  Word 2  |  Word 3  |   |\n",
    "|:------------------------------:|:----------:|:--------:|:--------:|:-:|\n",
    "| Doc 1 Word Topic Assignment--> |      1     |     2    |     1    |   |\n",
    "|           Document 1           |     eat    | broccoli |  banana  |   |\n",
    "| Doc 2 Word Topic Assignment--> |      2     |     1    |     2    |   |\n",
    "|           Document 2           |   banana   |  spinach |   lunch  |   |\n",
    "| Doc 3 Word Topic Assignment--> |      1     |     2    |     1    |   |\n",
    "|           Document 3           | chinchilla |  kitten  |   cute   |   |\n",
    "| Doc 4 Word Topic Assignment--> |      2     |     1    |     2    |   |\n",
    "|           Document 4           |   sister   |  kitten  |   today  |   |\n",
    "| Doc 5 Word Topic Assignment--> |      1     |     2    |     1    |   |\n",
    "|           Document 5           |   hamster  |    eat   | broccoli |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "The above is step 1.  For the purpose of this example, we simply randomly assign a topic to each word for each document.<br>\n",
    "\n",
    "M = 5 indicates that we have five documents total.<br>\n",
    "N =3 indicates that we have 3 word per document.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The list of unique words in our vocabulary (V):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Words (V = 11) |\n",
    "|----------------|\n",
    "| eat            |\n",
    "| broccoli       |\n",
    "| banana         |\n",
    "| spinach        |\n",
    "| lunch          |\n",
    "| chinchilla     |\n",
    "| kitten         |\n",
    "| cute           |\n",
    "| sister         |\n",
    "| today          |\n",
    "| hamster        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "The above is all the unique words in our vocabulary across all documents.<br>\n",
    "These are the words for which we will assign topics to based on our set of topics (k).<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the β (Beta) Distribution:\n",
    "<br>\n",
    "β — A distribution of words, one for each topic.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-xldj{border-color:inherit;text-align:left}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-xldj\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0lax\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">Words</td>\n",
    "    <td class=\"tg-0pky\">eat</td>\n",
    "    <td class=\"tg-0pky\">broccoli</td>\n",
    "    <td class=\"tg-0pky\">banana</td>\n",
    "    <td class=\"tg-0pky\">spinach</td>\n",
    "    <td class=\"tg-0pky\">lunch</td>\n",
    "    <td class=\"tg-0pky\">chinchilla</td>\n",
    "    <td class=\"tg-0pky\">kitten</td>\n",
    "    <td class=\"tg-0pky\">cute</td>\n",
    "    <td class=\"tg-0pky\">sister</td>\n",
    "    <td class=\"tg-0pky\">today</td>\n",
    "    <td class=\"tg-0lax\">hamster</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">Topics</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">1</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">2</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "To compute the Beta distribution, we look at our initial topic assignment for each word in each document.<br>\n",
    "\n",
    "We count the # of times each word is associated with a particular topic across all documents.<br>\n",
    "\n",
    "For example, we see here that the word \"eat\" appears two times in total.<br>\n",
    "The first time \"eat\" appears, it is associated with topic 1.<br>\n",
    "The second time \"eat\" appears, it is associated with topic 2.<br>\n",
    "\n",
    "Therefore, we put a 1 in the cell corresponding to Topic 1 and the Word \"eat\".<br>\n",
    "Therefore, we put a 1 in the cell corresponding to Topic 2 and the Word \"eat\".<br>\n",
    "\n",
    "We do this for each word (w) in our vocabulary (V) across all documents (d) based on our initial topic assignment for each word in each document.<br>\n",
    "\n",
    "Note: \"placeholder\" simply means that we are not inputting an actual value for the sake of simplicity of this example.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the θ (Theta) Distribution:\n",
    "<br>\n",
    "θ — A distribution of topics, one for each document.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-s268{text-align:left}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-s268\"></th>\n",
    "    <th class=\"tg-s268\">Documents</th>\n",
    "    <th class=\"tg-s268\">1</th>\n",
    "    <th class=\"tg-0lax\">2</th>\n",
    "    <th class=\"tg-0lax\">3</th>\n",
    "    <th class=\"tg-0lax\">4</th>\n",
    "    <th class=\"tg-0lax\">5</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s268\">Topics</td>\n",
    "    <td class=\"tg-s268\"></td>\n",
    "    <td class=\"tg-s268\"></td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s268\">1</td>\n",
    "    <td class=\"tg-s268\"></td>\n",
    "    <td class=\"tg-s268\">2</td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">2</td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "    <td class=\"tg-0lax\">2</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "To compute the Theta distribution, we look at our initial topic assignment for each word in each document.<br>\n",
    "\n",
    "We count the # of times each document is associated with each topic in our set of topics.<br>\n",
    "\n",
    "Since we have three words per document, we see that Document 1 is associated with Topic 1 two times since two words are associated with Topic 1.<br>\n",
    "We also see that Document 1 is associated with Topic 2 one time since one word is associated with Topic 2.<br>\n",
    "\n",
    "Therefore, we put a 2 in the cell corresponding to Topic 1 and Document 1.<br>\n",
    "Therefore, we put a 1 in the cell corresponding to Topic 2 and Document 1.<br>\n",
    "\n",
    "We do this for each topic (z) for each document (d) based on our initial topic assignment for each word in each document.<br>\n",
    "\n",
    "Note: \"placeholder\" simply means that we are not inputting an actual value for the sake of simplicity of this example.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the initial topic assignment for each word in each document:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-s268{text-align:left}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-s268\"></th>\n",
    "    <th class=\"tg-s268\"></th>\n",
    "    <th class=\"tg-s268\"></th>\n",
    "    <th class=\"tg-0lax\"></th>\n",
    "    <th class=\"tg-0lax\"></th>\n",
    "    <th class=\"tg-0lax\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\">Document 1</td>\n",
    "    <td class=\"tg-0lax\">Document 2</td>\n",
    "    <td class=\"tg-0lax\">Document 3</td>\n",
    "    <td class=\"tg-0lax\">Document 4</td>\n",
    "    <td class=\"tg-0lax\">Document 5</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Broccoli-Topic 1</td>\n",
    "    <td class=\"tg-0lax\">1 X 2 = 2</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Broccoli-Topic 2</td>\n",
    "    <td class=\"tg-0lax\">1 X 1 = 1</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "In order to update our initial topic assignments for each word in each document, we look at the Beta and Theta distributions we calculated above.<br>\n",
    "\n",
    "Notice that \"broccoli\" is associated with Topic 1 one time and Topic 2 one time in the Beta distribution.<br>\n",
    "Notice that Document 1 is associated with Topic 1 two times and Topic 2 one time in the Theta distribution.<br>\n",
    "\n",
    "Now, to calculate the new topic (z) assignment for the word (w) \"broccoli\", we do some simple arithmetic operations.<br>\n",
    "\n",
    "Look to the table above.<br>\n",
    "\n",
    "We multiply the value in the cell associated with Topic 1 and \"broccoli\" in the Beta distribution with the value in the cell associated with Topic 1 and Document 1 in the Theta distribution.  This gives us 1 X 2 = 2.<br>\n",
    "\n",
    "We then multiple the value in the cell associated with Topic 2 and \"broccoli\" in the Beta distribution with the value in the cell associated with Topic 2 and Document 1 in the Theta distribution.  This gives us 1 X 1 = 1.<br>\n",
    "\n",
    "Now, look to the table below for the next step.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-s268{text-align:left}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-s268\"></th>\n",
    "    <th class=\"tg-s268\"></th>\n",
    "    <th class=\"tg-s268\"></th>\n",
    "    <th class=\"tg-0lax\"></th>\n",
    "    <th class=\"tg-0lax\"></th>\n",
    "    <th class=\"tg-0lax\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\">Document 1</td>\n",
    "    <td class=\"tg-0lax\">Document 2</td>\n",
    "    <td class=\"tg-0lax\">Document 3</td>\n",
    "    <td class=\"tg-0lax\">Document 4</td>\n",
    "    <td class=\"tg-0lax\">Document 5</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Broccoli-Topic 1</td>\n",
    "    <td class=\"tg-0lax\">1 X 2 = 2 --&gt; 2 / (2 + 1) = 2/3</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Broccoli-Topic 2</td>\n",
    "    <td class=\"tg-0lax\">1 X 1 = 1 --&gt; 1 / (2 + 1) = 1/3</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Next, we sum the resulting values for the above multiplications to obtain (1 X 2) + (1 X 1) = 3.<br>\n",
    "\n",
    "Then, we divide the resulting values for each of the above multiplications by the value of the sum, 3.<br>\n",
    "\n",
    "Therefore, we have obtained probability values by which we use to select a new topic to assign to the word \"broccoli\".<br>\n",
    "\n",
    "In this case, they are a 2/3 = 0.6666667 chance that we assign \"broccoli\" to Topic 1 in Document 1 and a 1/3 = 0.33333333 chance that we assign \"broccoli\" to Topic 2 in Document 1.<br>\n",
    "\n",
    "Notice that we are assigning a new topic to the word \"broccoli\" in Document 1 according to PROBABILITIES that are calculated using the arithmetic operations above.<br>\n",
    "\n",
    "We are NOT simply arbitrarily assigning a new topic (z) to the word (w) \"broccoli\".  Everything is based on the Beta and Theta distributions and the conditional probabilities in the LDA pseudocode above.<br>\n",
    "\n",
    "We repeat this for each word (w) in our vocabulary (V) for each document (d).<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated topic assignment for \"broccoli\" in Document 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-c3ow\">Documents (M = 5, N = 3)</th>\n",
    "    <th class=\"tg-c3ow\">Word 1</th>\n",
    "    <th class=\"tg-c3ow\">Word 2</th>\n",
    "    <th class=\"tg-c3ow\">Word 3</th>\n",
    "    <th class=\"tg-c3ow\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Doc 1 Word Topic Assignment--&gt;</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\">2 --&gt; 1</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Document 1</td>\n",
    "    <td class=\"tg-c3ow\">eat</td>\n",
    "    <td class=\"tg-c3ow\">broccoli</td>\n",
    "    <td class=\"tg-c3ow\">banana</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Doc 2 Word Topic Assignment--&gt;</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Document 2</td>\n",
    "    <td class=\"tg-c3ow\">banana</td>\n",
    "    <td class=\"tg-c3ow\">spinach</td>\n",
    "    <td class=\"tg-c3ow\">lunch</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Doc 3 Word Topic Assignment--&gt;</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Document 3</td>\n",
    "    <td class=\"tg-c3ow\">chinchilla</td>\n",
    "    <td class=\"tg-c3ow\">kitten</td>\n",
    "    <td class=\"tg-c3ow\">cute</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Doc 4 Word Topic Assignment--&gt;</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Document 4</td>\n",
    "    <td class=\"tg-c3ow\">sister</td>\n",
    "    <td class=\"tg-c3ow\">kitten</td>\n",
    "    <td class=\"tg-c3ow\">today</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Doc 5 Word Topic Assignment--&gt;</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Document 5</td>\n",
    "    <td class=\"tg-c3ow\">hamster</td>\n",
    "    <td class=\"tg-c3ow\">eat</td>\n",
    "    <td class=\"tg-c3ow\">broccoli</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "Look to the table above for the new topic (z) assigned to the word (w) \"broccoli\" ASSUMING that in using the probabilities we just calculated we decide on reassigning \"broccoli\" to Topic 1 in Document 1.<br>\n",
    "\n",
    "It is important to know that we could also have assigned \"broccoli\" to Topic 2 instead.  However, based on the calculated probabilities for each topic (z) in our set of topics (k) it is far more likely that a randomized selection will select Topic 1 rather than Topic 2 (since Topic 1 = 2/3 chance and Topic 2 = 1/3 chance).<br>\n",
    "\n",
    "In an actual implementation of the LDA model, we would do this reassignment for each word (w) in each document (d) based on the probabilities calculated for each word (w) using the Beta and Theta distributions.<br>\n",
    "\n",
    "However, we are not done yet with just the first iteration of the LDA algorithm.<br>\n",
    "\n",
    "We still need to update the values for the Beta and Theta distributions for the next iteration of the LDA algorithm.<br>\n",
    "\n",
    "Refer below for these updates.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Updated β (Beta) Distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-xldj{border-color:inherit;text-align:left}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-xldj\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">Words</td>\n",
    "    <td class=\"tg-0pky\">eat</td>\n",
    "    <td class=\"tg-0pky\">broccoli</td>\n",
    "    <td class=\"tg-0pky\">banana</td>\n",
    "    <td class=\"tg-0pky\">spinach</td>\n",
    "    <td class=\"tg-0pky\">lunch</td>\n",
    "    <td class=\"tg-0pky\">chinchilla</td>\n",
    "    <td class=\"tg-0pky\">kitten</td>\n",
    "    <td class=\"tg-0pky\">cute</td>\n",
    "    <td class=\"tg-0pky\">sister</td>\n",
    "    <td class=\"tg-0pky\">today</td>\n",
    "    <td class=\"tg-0pky\">hamster</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">Topics</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">1</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1 --&gt; 2</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">2</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1 --&gt; 0</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Note that the cell associated with Topic 1 and \"broccoli\" has changed from 1 --> 2.<br>\n",
    "Note that the cell associated with Topic 2 and \"broccoli\" has change from 1 --> 0.<br>\n",
    "\n",
    "Refer to the updated topic assignment for \"broccoli\" in Document 1 in the table in the previous section.<br>\n",
    "\n",
    "In that table, notice that the word (w) \"broccoli\" is now only associated with Topic 1 across all documents (d).<br>\n",
    "In that table, notice that the word (w) \"broccoli\" occurs twice across all documents (d).<br>\n",
    "\n",
    "Therefore, we update the cell associated with Topic 1 and \"broccoli\" in the Beta distribution to 2.<br>\n",
    "Therefore, we update the cell associated with Topic 2 and \"broccoli\" in the Beta distribution to 0.<br>\n",
    "\n",
    "We would do this for all words (w) in our vocabulary (V) for all topics (z) in our set of topics (k).<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Updated θ (Theta) Distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-s268{text-align:left}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-s268\"></th>\n",
    "    <th class=\"tg-s268\">Documents</th>\n",
    "    <th class=\"tg-s268\">1</th>\n",
    "    <th class=\"tg-0lax\">2</th>\n",
    "    <th class=\"tg-0lax\">3</th>\n",
    "    <th class=\"tg-0lax\">4</th>\n",
    "    <th class=\"tg-0lax\">5</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s268\">Topics</td>\n",
    "    <td class=\"tg-s268\"></td>\n",
    "    <td class=\"tg-s268\"></td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s268\">1</td>\n",
    "    <td class=\"tg-s268\"></td>\n",
    "    <td class=\"tg-s268\">2 --&gt; 3</td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">2</td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\">1 --&gt; 0</td>\n",
    "    <td class=\"tg-0lax\">2</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Note that the cell associated with Topic 1 and Document 1 has changed from 2 --> 3.<br>\n",
    "Note that the cell associated with Topic 2 and Document 1 has changed from 1 --> 0.<br>\n",
    "\n",
    "Refer to the updated topic assignment for \"broccoli\" in Document 1 in the table in the previous section.<br>\n",
    "\n",
    "In that table, notice that Document 1 contains 3 words (N) that are now all associated with Topic 1.<br>\n",
    "So, there are now words in Document 1 that are associated with Topic 2.<br>\n",
    "\n",
    "Therefore, we update the cell associated with Topic 1 and Document 1 in the Theta distribution to 3.<br>\n",
    "Therefore, we update the cell assocaited with Topic 2 and Document 1 in the Theta distribution to 0.<br>\n",
    "\n",
    "We would do this for all documents (d) for all topics (z) in our set of topics (k).<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are finally finished with the FIRST iteration of the LDA algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Refer to the pseudocode for the LDA algorithm above and the explanation right below it.<br>\n",
    "\n",
    "We would now start at Step 2 and rinse + repeat until we run into time constraints.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Latent Dirichlet Allocation on SLO Twitter Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "Tweet preprocessing is done via a custom library imported as \"lda_util\" using \"slo_lda_topic_extraction_utility_functions.py\"<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Adjust log verbosity levels as necessary.<br>\n",
    "\n",
    "Set to \"DEBUG\" to view all debug output.<br>\n",
    "Set to \"INFO\" to view useful information on dataframe shape, etc.<br>\n",
    "\n",
    "Example of different debug levels (code-base currently utilizes only DEBUG and INFO levels):\n",
    "\n",
    "log.basicConfig(level=log.DEBUG)<br>\n",
    "log.basicConfig(level=log.INFO)<br>\n",
    "log.basicConfig(level=log.WARNING)<br>\n",
    "log.basicConfig(level=log.ERROR)<br>\n",
    "log.basicConfig(level=log.CRITICAL)<br>\n",
    "log.basicConfig(level=log.FATAL)<br>\n",
    "\n",
    "Refer to URL below for more information:\n",
    "\n",
    "https://docs.python.org/3/library/logging.html\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Resources Used:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation\n",
    "https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Import libraries.\n",
    "import logging as log\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import custom utility functions.\n",
    "import slo_lda_topic_extraction_utility_functions as lda_util\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Miscellaneous parameter adjustments for pandas and python.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "\"\"\"\n",
    "Turn log statements for various sections of code on/off.\n",
    "(adjust log level as necessary)\n",
    "\"\"\"\n",
    "log.basicConfig(level=log.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Refer to the \"Notes\" section at the bottom of this Jupyter Notebook for a link to the imported custom utility functions library.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and prep the dataset for use in LDA topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "This section of our code does the following:<br>\n",
    "\n",
    "1) Imports the dataset.<br>\n",
    "2) Reindex and shuffles the data in the dataset randomly using a numpy function.<br>\n",
    "3) Generates a Pandas dataframe to store the dataset.<br>\n",
    "4) Drops any NaN rows in the dataframe to avoid blowing CountVectorizer up.<br>\n",
    "5) Prints the shape and head of the dataframe.<br>\n",
    "6) Reindexes the dataframe.<br>\n",
    "7) Assigns column names to the dataframe columns.<br>\n",
    "8) Renames the dataframe to \"slo_feature_set\".<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "\n",
      "INFO:root:The shape of our preprocessed SLO dataframe with NaN (empty) rows dropped:\n",
      "INFO:root:(653094, 1)\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:The columns of our preprocessed SLO dataframe with NaN (empty) rows dropped:\n",
      "INFO:root:<bound method NDFrame.head of                                                   tweet_t\n",
      "404466    barnaby joyce endorses loan for adanis coalmine\n",
      "121811  asked the aust govt for a letter of suppo to h...\n",
      "648049  its perfectly simple we the voters of have sai...\n",
      "621027  ouch and the mine gone as well coal isnt good ...\n",
      "213299  so one day you accuse him of fence sitting the...\n",
      "...                                                   ...\n",
      "621502       coal will kill more people than world war ii\n",
      "109495  the market likes this reveals fuher shale oil ...\n",
      "162454  we were in bowen last week the ocean was black...\n",
      "614043  rally – billionaire polluter gautam meeting tu...\n",
      "328945  effects major job cuts at olympic dam copper mine\n",
      "\n",
      "[653094 rows x 1 columns]>\n",
      "INFO:root:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset.\n",
    "tweet_dataset_processed = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/datasets/dataset_20100101-20180510_tok_LDA_PROCESSED.csv\", sep=\",\")\n",
    "\n",
    "# Reindex and shuffle the data randomly.\n",
    "tweet_dataset_processed = tweet_dataset_processed.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_dataframe_processed = pd.DataFrame(tweet_dataset_processed)\n",
    "\n",
    "# Drop any NaN or empty Tweet rows in dataframe (or else CountVectorizer will blow up).\n",
    "tweet_dataframe_processed = tweet_dataframe_processed.dropna()\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(\"\\n\")\n",
    "log.info(\"The shape of our preprocessed SLO dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(tweet_dataframe_processed.shape)\n",
    "log.info(\"\\n\")\n",
    "log.info(\"The columns of our preprocessed SLO dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(tweet_dataframe_processed.head)\n",
    "log.info(\"\\n\")\n",
    "\n",
    "# Reindex everything.\n",
    "tweet_dataframe_processed.index = pd.RangeIndex(len(tweet_dataframe_processed.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_dataframe_processed_column_names = ['Tweet']\n",
    "\n",
    "# Rename column in dataframe.\n",
    "tweet_dataframe_processed.columns = tweet_dataframe_processed_column_names\n",
    "\n",
    "# Create input feature.\n",
    "selected_features = tweet_dataframe_processed[tweet_dataframe_processed_column_names]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "# Check what we are using as inputs.\n",
    "log.debug(\"\\n\")\n",
    "log.debug(\"The Tweets in our input feature:\")\n",
    "log.debug(processed_features['Tweet'])\n",
    "log.debug(\"\\n\")\n",
    "\n",
    "# Create feature set.\n",
    "slo_feature_set = processed_features['Tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "The above log.INFO messages depict the shape and contents of the preprocessed dataframe after dropping any rows that are just \"NaN\", indicating the Tweet was full of irrelevant words and is now empty due to removal of those irrelevant words.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "We use the Scikit-Learn CountVectorizer class to vectorize our categorical Tweet data.<br>\n",
    "\n",
    "We use the Scikit-Learn LatentDirichletAllocation class with the below hyper parameters to train on and fit to our Tweet data.<br>\n",
    "\n",
    "We use a utility function to display Topics 1-20 and the top 10 words associated with each Topic.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "tax pay energy thanks ceo latest high corporate story office\n",
      "Topic 1:\n",
      "labor australian foescue going federal election said giant political hey\n",
      "Topic 2:\n",
      "stop coal rail fund line clean seam gas protest global\n",
      "Topic 3:\n",
      "news make right lnp wants did thats local shares watch\n",
      "Topic 4:\n",
      "climate public money change fight business world national policy doing\n",
      "Topic 5:\n",
      "jobs new coal iron ore plans beach create really end\n",
      "Topic 6:\n",
      "time year mines paid industry price look australias companies profit\n",
      "Topic 7:\n",
      "turnbull funding oil years banks biggest planet cuts despite dead\n",
      "Topic 8:\n",
      "queensland india coal power cou minister alp jobs approval environment\n",
      "Topic 9:\n",
      "ahead adani council come carbon politicians fossil week production mega\n",
      "Topic 10:\n",
      "project gas slocashn narrabri vote getting gov noh pipeline sign\n",
      "Topic 11:\n",
      "water say basin galilee way barnaby joyce repo canavan let\n",
      "Topic 12:\n",
      "want know coal state group campaign oppose join forest tell\n",
      "Topic 13:\n",
      "government environmental labor cut loan job disaster veto coalition goes\n",
      "Topic 14:\n",
      "coal future adanis point deal build po taxpayer pm massive\n",
      "Topic 15:\n",
      "loan govt need wont greens indian naif use premier board\n",
      "Topic 16:\n",
      "company day billion help breaking huge native title bank times\n",
      "Topic 17:\n",
      "suppo dont coal good shoen does doesnt think community money\n",
      "Topic 18:\n",
      "reef people barrier coal save country green demand funds message\n",
      "Topic 19:\n",
      "adanis farmers action risk coalmine work land water free case\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model.\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(slo_feature_set)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# Run LDA.\n",
    "lda = LatentDirichletAllocation(n_topics=20, max_iter=5, learning_method='online', learning_offset=50.,\n",
    "                                random_state=0).fit(tf)\n",
    "\n",
    "# Display the top words for each topic.\n",
    "lda_util.display_topics(lda, tf_feature_names, 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "We cannot seem to find any strong correlation between the 10 words in each Topic such that we could assign an English descriptor to each topics, such as \"economic\", \"environmental\", \"social\", etc.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from a different execution of LDA topic extraction on our dataset (using PyCharm):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "These results were obtained using the exact same code-base and hyper parameters, only it was done within PyCharm rather than the Jupyter Notebook.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Topic 0:\n",
    "loan rail lnp federal line fund think naif use story\n",
    "Topic 1:\n",
    "need group community carbon country stand high week end wrong\n",
    "Topic 2:\n",
    "tax breaking paid industry ceo green china financial hey licence\n",
    "Topic 3:\n",
    "government new money day environmental barnaby joyce taxpayers plans giving\n",
    "Topic 4:\n",
    "news funding foescue shoen help thanks pm beach bank cut\n",
    "Topic 5:\n",
    "suppo going world wont greens work vote let companies dollars\n",
    "Topic 6:\n",
    "time farmers year look global fossil profit video police best\n",
    "Topic 7:\n",
    "people say campaign free indian giant local australias corruption prices\n",
    "Topic 8:\n",
    "gas coal public slocashn project deal does adani risk biggest\n",
    "Topic 9:\n",
    "action fight state did join decision protest doesnt getting read\n",
    "Topic 10:\n",
    "climate project coal change repo environment price shares doing lost\n",
    "Topic 11:\n",
    "minister election build years native title canavan away planet owners\n",
    "Topic 12:\n",
    "reef turnbull cou barrier coal ahead know adanis wants approval\n",
    "Topic 13:\n",
    "india company pay coal alp business right oil oppose win\n",
    "Topic 14:\n",
    "coal good power make mines iron ore new po protect\n",
    "Topic 15:\n",
    "jobs australian needs create thats billion tell got investment workers\n",
    "Topic 16:\n",
    "adanis energy future coal land taxpayer said clean destroy gov\n",
    "Topic 17:\n",
    "want govt dont point coal massive politicians infrastructure protesters subsidies\n",
    "Topic 18:\n",
    "stop water queensland narrabri coal disaster corporate unlimited live tonight\n",
    "Topic 19:\n",
    "labor coalmine coal galilee way huge basin job pollution townsville\n",
    "INFO:root:The time taken to perform the operation is: \n",
    "INFO:root:452.49793434143066\n",
    "INFO:root:\n",
    "\n",
    "\n",
    "Process finished with exit code 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Though the results are different, the top 10 words for each of the 20 different Topics still lack any strong association to each other.  We still cannot easily assign any English descriptors to each topic.<br>\n",
    "\n",
    "We decided to time the LDA model.  It takes around 450 seconds or so to finish LDA topic extraction per execution, so it is not a particularly fast process.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from LDA topic extraction for 3 topics (using PyCharm):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Topic 0:\n",
    "coal tax loan australian govt money pay turnbull adanis government\n",
    "Topic 1:\n",
    "coal stop labor reef want climate time people dont need\n",
    "Topic 2:\n",
    "coal jobs gas water project adanis rail news cou farmers\n",
    "INFO:root:The time taken to perform the operation is: \n",
    "INFO:root:713.6182627677917\n",
    "INFO:root:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "  \n",
    "Again, we can't really discern any noticeable patterns among the top 10 words for each topics.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from LDA topic extraction for 6 topics (using PyCharm):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Topic 0:\n",
    "coal queensland money dont adanis rail funding loan fund government\n",
    "Topic 1:\n",
    "gas people coal tax project minister paid narrabri iron news\n",
    "Topic 2:\n",
    "labor coal stop india loan time pay federal year greens\n",
    "Topic 3:\n",
    "coal new reef water company point slocashn barrier adanis make\n",
    "Topic 4:\n",
    "climate good lnp world basin change ahead say turnbull alp\n",
    "Topic 5:\n",
    "jobs coal suppo govt need power foescue going cou action\n",
    "INFO:root:The time taken to perform the operation is: \n",
    "INFO:root:506.35622787475586\n",
    "INFO:root:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "As stated above.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from LDA topic extraction for 12 topics (using PyCharm):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Topic 0:\n",
    "coal power india environmental galilee fight repo po plans join\n",
    "Topic 1:\n",
    "fund know billion wants business years pm giant really cut\n",
    "Topic 2:\n",
    "water breaking protect local cou basin doesnt slomentions high licence\n",
    "Topic 3:\n",
    "gas coal new news people oil huge seam canavan renewables\n",
    "Topic 4:\n",
    "labor going foescue shoen help election iron ore said group\n",
    "Topic 5:\n",
    "jobs loan australian dont want government point money ahead greens\n",
    "Topic 6:\n",
    "stop coal say make world action risk work biggest people\n",
    "Topic 7:\n",
    "need year day deal ceo banks come look politicians getting\n",
    "Topic 8:\n",
    "reef coal farmers land barrier mines thanks new use indian\n",
    "Topic 9:\n",
    "adanis queensland turnbull govt good company lnp right labor coalmine\n",
    "Topic 10:\n",
    "project coal suppo climate rail public adanis change federal funding\n",
    "Topic 11:\n",
    "tax pay time energy future paid clean beach campaign oppose\n",
    "INFO:root:The time taken to perform the operation is: \n",
    "INFO:root:468.9760229587555\n",
    "INFO:root:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "ditto.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive grid search for Scikit-Learn LDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "We use Scikit-Learn's Pipeline Class to construct a pipeline consisting of the CounteVectorizer and LatentDirichletAllocation classes.<br>\n",
    "\n",
    "The \"parameters\" dictionary determine all the possible combinations of hyper parameters we will test in order to find the optimal hyper parameters for the Scikit-Learn LDA model.<br>\n",
    "\n",
    "The grid search is performed by fitting on the data we wish to use for topic extraction.<br>\n",
    "\n",
    "The optimal hyper parameters are displayed via \"log.info\" messages so the log verbosity level must be set appropriately to view them.<br>\n",
    "\n",
    "The function to perform exhaustive grid search isn't currently used.  We will use it in the future once we fully understand LDA, its associated hyper parameters, and how to tune for improved results.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Construct the pipeline.\n",
    "latent_dirichlet_allocation_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')),\n",
    "    ('clf', LatentDirichletAllocation()),\n",
    "])\n",
    "\n",
    "# What parameters do we search for?\n",
    "parameters = {\n",
    "#     'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "    'clf__n_components': [1, 5, 10, 15],\n",
    "    'clf__doc_topic_prior': [None],\n",
    "    'clf__topic_word_prior': [None],\n",
    "    'clf__learning_method': ['batch', 'online'],\n",
    "#     'clf__learning_decay': [0.5, 0.7, 0.9],\n",
    "#     'clf__learning_offset': [5, 10, 15],\n",
    "#     'clf__max_iter': [5, 10, 15],\n",
    "#     'clf__batch_size': [64, 128, 256],\n",
    "#     'clf__evaluate_every': [0],\n",
    "#     'clf__total_samples': [1e4, 1e6, 1e8],\n",
    "#     'clf__perp_tol': [1e-1, 1e-2, 1e-3],\n",
    "#     'clf__mean_change_tol': [1e-1, 1e-3, 1e-5],\n",
    "#     'clf__max_doc_update_iter': [50, 100, 150],\n",
    "    'clf__n_jobs': [-1],\n",
    "    'clf__verbose': [0],\n",
    "    'clf__random_state': [None],\n",
    "}\n",
    "\n",
    "# Perform the grid search.\n",
    "latent_dirichlet_allocation_clf = GridSearchCV(latent_dirichlet_allocation_clf, parameters, cv=5, iid=False,\n",
    "                                               n_jobs=-1)\n",
    "latent_dirichlet_allocation_clf.fit(slo_feature_set)\n",
    "\n",
    "# View all the information stored in the model after training it.\n",
    "classifier_results = pd.DataFrame(latent_dirichlet_allocation_clf.cv_results_)\n",
    "log.debug(\"The shape of the Latent Dirichlet Allocation model's result data structure is:\")\n",
    "log.debug(classifier_results.shape)\n",
    "log.debug(\n",
    "    \"The contents of the Latent Dirichlet Allocation model's result data structure is:\")\n",
    "log.debug(classifier_results.head())\n",
    "\n",
    "# Display the optimal parameters.\n",
    "log.info(\"The optimal parameters found for the Latent Dirichlet Allocation is:\")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    log.info(\"%s: %r\" % (param_name, latent_dirichlet_allocation_clf.best_params_[param_name]))\n",
    "log.info(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "If running this code snippet, you may want to change \"n_jobs=-1\" to \"n_jobs=0\" to prevent Python from utilizing all CPU cores and bogging down your system to unusability.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Topic Extraction using the \"lda\" library and collapsed Gibbs Sampling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "The code below uses the \"lda\" library package that performs LDA topic extraction using collapsed Gibbs Sampling.<br>\n",
    "This is different from the Scikit-Learn implementation that uses online variational inference.<br>\n",
    "Otherwise, the dataset is the same and we are still using Scikit-Learn's CountVectorizer class to vectorize our data.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 653094\n",
      "INFO:lda:vocab_size: 1000\n",
      "INFO:lda:n_words: 3267212\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -33566609\n",
      "INFO:lda:<10> log likelihood: -27759459\n",
      "INFO:lda:<20> log likelihood: -24357244\n",
      "INFO:lda:<30> log likelihood: -23217301\n",
      "INFO:lda:<40> log likelihood: -22887465\n",
      "INFO:lda:<50> log likelihood: -22760021\n",
      "INFO:lda:<60> log likelihood: -22691980\n",
      "INFO:lda:<70> log likelihood: -22648547\n",
      "INFO:lda:<80> log likelihood: -22621636\n",
      "INFO:lda:<90> log likelihood: -22601056\n",
      "INFO:lda:<99> log likelihood: -22580145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: labor stop greens vote alp lnp election shoen suppo win\n",
      "Topic 1: time good going know im really think did dont right\n",
      "Topic 2: beach day people join watch action tour stop bob sydney\n",
      "Topic 3: coal adanis cou point approval federal new green giant light\n",
      "Topic 4: jobs create thousands tourism coal 10000 pm adanis job cou\n",
      "Topic 5: reef coal barrier stop save turnbull coral indian canavan minister\n",
      "Topic 6: tax paid energy australian pay ceo companies donations origin chevron\n",
      "Topic 7: gas project narrabri seam coal forest dam barnaby water joyce\n",
      "Topic 8: climate change future coal energy clean fossil time planet global\n",
      "Topic 9: iron ore foescue oil shares production price prices profit year\n",
      "Topic 10: coal money fund banks funding billion adanis taxpayers project govt\n",
      "Topic 11: tax pay company corporate workers cut use profits debt cuts\n",
      "Topic 12: coal power india new solar mines environmental record company renewables\n",
      "Topic 13: coal australian fuher foescue creek assets stranded asset project maules\n",
      "Topic 14: water farmers basin free licence unlimited aesian groundwater coal suppo\n",
      "Topic 15: want coal dont suppo doesnt need project does shoen adanis\n",
      "Topic 16: land stop native title owners people traditional townsville indigenous rights\n",
      "Topic 17: rail line coal loan galilee adanis basin public funding money\n",
      "Topic 18: loan government turnbull govt naif labor slocashn deal taxpayer queensland\n",
      "Topic 19: coal thanks action latest times council point protesters po work\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import lda\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model.\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(slo_feature_set)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# Train and fit the LDA model.\n",
    "model = lda.LDA(n_topics=20, n_iter=100, random_state=1)\n",
    "model.fit(tf)  # model.fit_transform(X) is also available\n",
    "topic_word = model.topic_word_  # model.components_ also works\n",
    "n_top_words = 10\n",
    "\n",
    "# Display the topics and the top words associated with.\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(tf_feature_names)[np.argsort(topic_dist)][:-(n_top_words + 1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "The results seem to be as incoherent as the Scikit-Learn implementation of LDA topic extraction using online variational inference.<br>\n",
    "\n",
    "It's difficult to see any correlation between the 10 top words for each topic.<br>\n",
    "\n",
    "Here, we are using n_iter=100 (iterations) as the fitting to our Twitter data is a lot faster than the Scikit-Learn implementation where max_iter=5 already takes 450 seconds.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A second set of LDA topic extraction results using the \"lda\" library (in Pycharm) with 1000 iterations:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INFO:lda:n_documents: 653094\n",
    "INFO:lda:vocab_size: 1000\n",
    "INFO:lda:n_words: 3267212\n",
    "INFO:lda:n_topics: 20\n",
    "INFO:lda:n_iter: 1000\n",
    "WARNING:lda:all zero row in document-term matrix found\n",
    "INFO:lda:<0> log likelihood: -33566800\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "INFO:lda:<999> log likelihood: -22482922\n",
    "Topic 0: project coal australian banks bank fund funding adanis finance chinese\n",
    "Topic 1: tax paid pay company corporate energy companies islands cayman australian\n",
    "Topic 2: beach day watch tour video iluka people got work power\n",
    "Topic 3: thanks council townsville protest action latest times people outside protesters\n",
    "Topic 4: rail loan line adanis coal galilee naif basin veto queensland\n",
    "Topic 5: labor stop greens vote alp lnp election shoen want win\n",
    "Topic 6: people land owners traditional farmers coal indigenous creek site local\n",
    "Topic 7: loan canavan minister board matt news abc coalition resources john\n",
    "Topic 8: coal suppo labor want oppose shoen dont poll doesnt voters\n",
    "Topic 9: reef barrier coal stop people destroy save future need protect\n",
    "Topic 10: foescue iron ore oil shares price production prices profit metals\n",
    "Topic 11: dont going good think want really make know need look\n",
    "Topic 12: coal turnbull native title adanis indian government stop point india\n",
    "Topic 13: environmental coal cou disaster dam approval australian federal financial adanis\n",
    "Topic 14: jobs create thousands 10000 coal tourism job cou queensland pm\n",
    "Topic 15: coal money billion govt want fund public turnbull taxpayers taxpayer\n",
    "Topic 16: coal new power india point solar mines adanis po time\n",
    "Topic 17: water free basin farmers unlimited aesian licence groundwater queensland coal\n",
    "Topic 18: climate coal change future energy clean fossil time carbon global\n",
    "Topic 19: gas project coal narrabri barnaby joyce seam water field forest\n",
    "INFO:root:The time taken to perform the operation is: \n",
    "INFO:root:522.370343208313\n",
    "INFO:root:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "These results are from another execution using the same library as the previous results.<br>\n",
    "\n",
    "Again, there's no discernible patterns in the choice of top words across all Topics.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A third set of LDA topic extraction results using the \"lda\" library (in Pycharm) with 1000 iterations and 3 topics:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INFO:lda:n_documents: 653094\n",
    "INFO:lda:vocab_size: 1000\n",
    "INFO:lda:n_words: 3267212\n",
    "INFO:lda:n_topics: 3\n",
    "INFO:lda:n_iter: 1000\n",
    "WARNING:lda:all zero row in document-term matrix found\n",
    "INFO:lda:<0> log likelihood: -26923029\n",
    ".\n",
    ".\n",
    ".\n",
    "INFO:lda:<999> log likelihood: -21700037\n",
    "Topic 0: coal water gas climate reef people stop project adanis farmers\n",
    "Topic 1: coal tax new foescue australian company energy india iron power\n",
    "Topic 2: coal jobs labor loan adanis want money government turnbull suppo\n",
    "INFO:root:The time taken to perform the operation is: \n",
    "INFO:root:212.95609331130981\n",
    "INFO:root:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Same situation.  Difficult to discern any patterns among the top words chosen for each topics.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A fourth set of LDA topic extraction results using the \"lda\" library (in Pycharm) with 1000 iterations and 6 topics:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INFO:lda:n_documents: 653094\n",
    "INFO:lda:vocab_size: 1000\n",
    "INFO:lda:n_words: 3267212\n",
    "INFO:lda:n_topics: 6\n",
    "INFO:lda:n_iter: 1000\n",
    "WARNING:lda:all zero row in document-term matrix found\n",
    "INFO:lda:<0> log likelihood: -30452684\n",
    ".\n",
    ".\n",
    ".\n",
    "INFO:lda:<999> log likelihood: -21989143\n",
    "Topic 0: coal jobs reef barrier energy adanis new future climate need\n",
    "Topic 1: tax foescue coal paid iron ore australian company oil price\n",
    "Topic 2: labor coal want stop suppo shoen greens lnp dont alp\n",
    "Topic 3: coal loan adanis money rail government fund funding public taxpayer\n",
    "Topic 4: coal gas water project farmers narrabri cou land seam native\n",
    "Topic 5: climate coal action people beach day thanks change time join\n",
    "INFO:root:The time taken to perform the operation is: \n",
    "INFO:root:291.4145665168762\n",
    "INFO:root:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Ditto.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A fifth set of LDA topic extraction results using the \"lda\" library (in Pycharm) with 1000 iterations and 12 topics:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INFO:lda:n_documents: 653094\n",
    "INFO:lda:vocab_size: 1000\n",
    "INFO:lda:n_words: 3267212\n",
    "INFO:lda:n_topics: 12\n",
    "INFO:lda:n_iter: 1000\n",
    "WARNING:lda:all zero row in document-term matrix found\n",
    "INFO:lda:<0> log likelihood: -32585851\n",
    ".\n",
    ".\n",
    ".\n",
    "INFO:lda:<999> log likelihood: -22256834\n",
    "Topic 0: reef coal climate barrier future change energy world stop clean\n",
    "Topic 1: foescue iron ore oil shares price ceo news production new\n",
    "Topic 2: labor greens stop want lnp alp suppo vote shoen dont\n",
    "Topic 3: water basin barnaby coal joyce galilee rail free farmers thanks\n",
    "Topic 4: coal money want fund adanis govt banks billion turnbull funding\n",
    "Topic 5: gas coal project water narrabri seam farmers forest pipeline field\n",
    "Topic 6: coal cou stop adanis native title land federal coalmine approval\n",
    "Topic 7: loan coal naif rail queensland minister government line public funding\n",
    "Topic 8: coal new india power point adanis mines solar po project\n",
    "Topic 9: action beach people day join time work watch protest morning\n",
    "Topic 10: tax paid pay company australian companies corporate govt adanis energy\n",
    "Topic 11: jobs create coal dam thousands 10000 disaster tourism reef job\n",
    "INFO:root:The time taken to perform the operation is: \n",
    "INFO:root:404.1787123680115\n",
    "INFO:root:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "And Ditto.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does it work poorly on Tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "##### Based on Derek Fisher's senior project presentation:\n",
    "\n",
    "1) LDA typically works best when the documents are lengthy (large word count) and written in a formal proper style.\n",
    "\n",
    "2) Tweet text is generally very short in length with a max of around 280 characters.\n",
    "\n",
    "3) Tweet text is generally written very informally style-wise.\n",
    "\n",
    "    i) emojis.\n",
    "    ii) spelling errors.\n",
    "    iii) other grammatical errors.\n",
    "    iv) etc.\n",
    "\n",
    "4) The above makes it difficult for the LDA algorithm to discover any prominent underlying hidden structures.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "## Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Refer to the below URL link to the utility functions used above for data preprocessing and LDA topic extraction:\n",
    "\n",
    "https://github.com/J-Jinn/Summer-Research-2019/blob/master/slo_lda_topic_extraction_utility_functions.py\n",
    "\n",
    "TODO - implement NMF - Non-Negative Matrix Factorization for Topic Modeling\n",
    "\n",
    "https://stackabuse.com/python-for-nlp-topic-modeling/\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
