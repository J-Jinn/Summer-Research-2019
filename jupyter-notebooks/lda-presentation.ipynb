{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Adapted SLO TBL topic classification codebase and Derek Fisher's code for my own LDA topic extraction implemenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources Used:\n",
    "\n",
    "https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158\n",
    "\n",
    "https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n",
    "\n",
    "https://www.investopedia.com/terms/p/posterior-probability.asp\n",
    "\n",
    "https://cs.calvin.edu/courses/cs/x95/videos/2018-2019/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Basics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "##### Basic Concept:\n",
    "\n",
    "Each document described by a distribution of topics.<br>\n",
    "Each topic described by a distribution of words.<br>\n",
    "Typically uses bag-of-words feature representation for documents.<br>\n",
    "Permits the identification of topics within documents and the mapping of documents to associated topics.<br>\n",
    "\n",
    "##### Terms:\n",
    "\n",
    "Observed layer: documents (composites) and words (parts).<br>\n",
    "Hidden (latent) layer: topics (categories).<br>\n",
    "\n",
    "k — Number of topics a document belongs to (a fixed number).<br>\n",
    "\n",
    "V — Size of the vocabulary.<br>\n",
    "\n",
    "M — Number of documents.<br>\n",
    "\n",
    "N — Number of words in each document.<br>\n",
    "\n",
    "w — A word in a document. This is represented as a one hot encoded vector of size V (i.e. V — vocabulary size).<br>\n",
    "\n",
    "w (bold w): represents a document (i.e. vector of “w”s) of N words.<br>\n",
    "\n",
    "D — Corpus, a collection of M documents.<br>\n",
    "\n",
    "z — A topic from a set of k topics. A topic is a distribution words. For example it might be, Animal = (0.3 Cats, 0.4 Dogs, 0 AI, 0.2 Loyal, 0.1 Evil).<br>\n",
    "\n",
    "![lda](lda_model.jpeg)\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "α — Distribution related parameter that governs what the distribution of topics is for all the documents in the corpus looks like.<br>\n",
    "\n",
    "θ — Random matrix where θ(i,j) represents the probability of the i th document to containing the j th topic.<br>\n",
    "\n",
    "η — Distribution related parameter that governs what the distribution of words in each topic looks like.<br>\n",
    "\n",
    "β — A random matrix where β(i,j) represents the probability of i th topic containing the j th word.<br>\n",
    "\n",
    "##### Dirichlet Distribution (example):\n",
    "\n",
    "![dirichlet](dirichlet_distribution.png)\n",
    "\n",
    "1) Large values of α pushes the distribution to the center.<br>\n",
    "2) Small vlues of α pushes the distribution to the edges.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "##### Mathematical equivalent of the above graphical representation of LDA:\n",
    "\n",
    "![mathematical_model](lda_equation.png)\n",
    "\n",
    "##### English Translation:\n",
    "\n",
    "Given a set of M documents with each containing N words and each word generated from a topic \"k\" from a set of K topics, find the joint posterior probability of:\n",
    "\n",
    "θ — A distribution of topics, one for each document,<br>\n",
    "z — N Topics for each document,<br>\n",
    "β — A distribution of words, one for each topic,<br>\n",
    "\n",
    "Given:\n",
    "\n",
    "D — All the data we have (i.e. the corups),<br>\n",
    "\n",
    "Using the parameters:\n",
    "\n",
    "α — A parameter vector for each document (document — Topic distribution).<br>\n",
    "η — A parameter vector for each topic (topic — word distribution).<br>\n",
    "\n",
    "\n",
    "##### Joint posterior probability: \n",
    "\n",
    "In Bayesian statistics, it is the revised or updated probablity of an event occurring given new information.<br>\n",
    "Calculated by updating the prior probability using Bayes' Theorem.<br>\n",
    "In other words, conditional probability - probability of event A occurring given that event B has occurred.<br>\n",
    "\n",
    "##### Prior probability:\n",
    "\n",
    "In Bayesian statistics, it is the probablity of an event occurring before new information is given.<br>\n",
    "Calculated using Bayes' Theorem.\n",
    "\n",
    "##### Note:\n",
    "\n",
    "Choose # of topics < # of documents to reduce dimensionality for further analysis using other algorithms.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of using a LDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the datasets to be used in the LDA model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SLO Topic Modeling\n",
    "Advisor: Professor VanderLinden\n",
    "Name: Joseph Jinn\n",
    "Date: 5-29-19\n",
    "\n",
    "LDA - Latent Dirichlet Allocation\n",
    "\n",
    "###########################################################\n",
    "Notes:\n",
    "\n",
    "LDA can only use raw term counts (CANNOT use tfidf transformer)\n",
    "\n",
    "###########################################################\n",
    "Resources Used:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation\n",
    "\n",
    "https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "\n",
    "import logging as log\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Note: Need to set level AND turn on debug variables in order to see all debug output.\n",
    "log.basicConfig(level=log.DEBUG)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Miscellaneous parameter adjustments for pandas and python.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "\"\"\"\n",
    "Turn debug log statements for various sections of code on/off.\n",
    "\"\"\"\n",
    "# Debug the GridSearch functions for each Classifier.\n",
    "debug_pipeline = True\n",
    "# Debug the initial dataset import and feature/target set creation.\n",
    "debug_preprocess_tweets = False\n",
    "# Debug create_training_and_test_set() function.\n",
    "debug_train_test_set_creation = False\n",
    "\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "\n",
    "# Import the datasets.\n",
    "tweet_dataset_processed1 = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/datasets/tbl_kvlinden_LDA_PROCESSED.csv\", sep=\",\")\n",
    "\n",
    "tweet_dataset_processed2 = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/datasets/tbl_training_set_LDA_PROCESSED.csv\", sep=\",\")\n",
    "\n",
    "tweet_dataset_processed3 = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/datasets/dataset_20100101-20180510_tok_LDA_PROCESSED.csv\", sep=\",\")\n",
    "\n",
    "# Reindex and shuffle the data randomly.\n",
    "tweet_dataset_processed1 = tweet_dataset_processed1.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed1.index))\n",
    "\n",
    "tweet_dataset_processed2 = tweet_dataset_processed2.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed2.index))\n",
    "\n",
    "tweet_dataset_processed3 = tweet_dataset_processed3.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed3.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_dataframe_processed1 = pd.DataFrame(tweet_dataset_processed1)\n",
    "tweet_dataframe_processed2 = pd.DataFrame(tweet_dataset_processed2)\n",
    "tweet_dataframe_processed3 = pd.DataFrame(tweet_dataset_processed3)\n",
    "\n",
    "if debug_preprocess_tweets:\n",
    "    # Print shape and column names.\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The shape of our SLO dataframe 1:\")\n",
    "    log.debug(tweet_dataframe_processed1.shape)\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The columns of our SLO dataframe 1:\")\n",
    "    log.debug(tweet_dataframe_processed1.head)\n",
    "    log.debug(\"\\n\")\n",
    "    # Print shape and column names.\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The shape of our SLO dataframe 2:\")\n",
    "    log.debug(tweet_dataframe_processed2.shape)\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The columns of our SLO dataframe 2:\")\n",
    "    log.debug(tweet_dataframe_processed2.head)\n",
    "    log.debug(\"\\n\")\n",
    "    # Print shape and column names.\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The shape of our SLO dataframe 3:\")\n",
    "    log.debug(tweet_dataframe_processed3.shape)\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The columns of our SLO dataframe 3:\")\n",
    "    log.debug(tweet_dataframe_processed3.head)\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "# Rename column in 3rd dataframe for concatenation purposes.\n",
    "tweet_dataframe_processed3.columns = ['Tweet']\n",
    "\n",
    "# Drop any NaN or empty Tweet rows in 3rd dataframe (or else CountVectorizer will blow up).\n",
    "tweet_dataframe_processed3 = tweet_dataframe_processed3.dropna()\n",
    "\n",
    "# Concatenate the individual datasets together.\n",
    "frames = [tweet_dataframe_processed1, tweet_dataframe_processed2, tweet_dataframe_processed3]\n",
    "slo_dataframe_combined = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "if debug_preprocess_tweets:\n",
    "    # Print shape and column names.\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The shape of our SLO dataframe combined:\")\n",
    "    log.debug(slo_dataframe_combined.shape)\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The columns of our SLO dataframe combined:\")\n",
    "    log.debug(slo_dataframe_combined.head)\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "# Reindex everything.\n",
    "slo_dataframe_combined.index = pd.RangeIndex(len(slo_dataframe_combined.index))\n",
    "# slo_dataframe_combined.index = range(len(slo_dataframe_combined.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_dataframe_processed_column_names = ['Tweet']\n",
    "\n",
    "# Create input features.\n",
    "selected_features = slo_dataframe_combined[tweet_dataframe_processed_column_names]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "if debug_preprocess_tweets:\n",
    "    # Check what we are using as inputs.\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The Tweets in our input feature:\")\n",
    "    log.debug(processed_features['Tweet'])\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "# Create feature set.\n",
    "slo_feature_set = processed_features['Tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive grid search for Scikit-Learn LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_dirichlet_allocation_grid_search():\n",
    "    \"\"\"\n",
    "    Function performs exhaustive grid search for LDA.\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "    # Construct the pipeline.\n",
    "    latent_dirichlet_allocation_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')),\n",
    "        ('clf', LatentDirichletAllocation()),\n",
    "    ])\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # What parameters do we search for?\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "        'clf__n_components': [1, 5, 10, 15],\n",
    "        'clf__doc_topic_prior': [None],\n",
    "        'clf__topic_word_prior': [None],\n",
    "        'clf__learning_method': ['batch', 'online'],\n",
    "        'clf__learning_decay': [0.5, 0.7, 0.9],\n",
    "        'clf__learning_offset': [5, 10, 15],\n",
    "        'clf__max_iter': [5, 10, 15],\n",
    "        'clf__batch_size': [64, 128, 256],\n",
    "        'clf__evaluate_every': [0],\n",
    "        'clf__total_samples': [1e4, 1e6, 1e8],\n",
    "        'clf__perp_tol': [1e-1, 1e-2, 1e-3],\n",
    "        'clf__mean_change_tol': [1e-1, 1e-3, 1e-5],\n",
    "        'clf__max_doc_update_iter': [50, 100, 150],\n",
    "        'clf__n_jobs': [-1],\n",
    "        'clf__verbose': [0],\n",
    "        'clf__random_state': [None],\n",
    "    }\n",
    "\n",
    "    # Perform the grid search.\n",
    "    latent_dirichlet_allocation_clf = GridSearchCV(latent_dirichlet_allocation_clf, parameters, cv=5, iid=False,\n",
    "                                                   n_jobs=-1)\n",
    "    latent_dirichlet_allocation_clf.fit(slo_feature_set)\n",
    "\n",
    "    if debug_pipeline:\n",
    "        # View all the information stored in the model after training it.\n",
    "        classifier_results = pd.DataFrame(latent_dirichlet_allocation_clf.cv_results_)\n",
    "        log.debug(\"The shape of the Latent Dirichlet Allocation model's result data structure is:\")\n",
    "        log.debug(classifier_results.shape)\n",
    "        log.debug(\n",
    "            \"The contents of the Latent Dirichlet Allocation model's result data structure is:\")\n",
    "        log.debug(classifier_results.head())\n",
    "\n",
    "    # Display the optimal parameters.\n",
    "    log.debug(\"The optimal parameters found for the Latent Dirichlet Allocation is:\")\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        log.debug(\"%s: %r\" % (param_name, latent_dirichlet_allocation_clf.best_params_[param_name]))\n",
    "    log.debug(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions that performs the topic extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_dirichlet_allocation_topic_extraction():\n",
    "    \"\"\"\n",
    "    Function performs topic extraction on Tweets using LDA.\n",
    "\n",
    "    :return: none.\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model.\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(slo_feature_set)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    # Run LDA.\n",
    "    lda = LatentDirichletAllocation(n_topics=20, max_iter=5, learning_method='online', learning_offset=50.,\n",
    "                                    random_state=0).fit(tf)\n",
    "\n",
    "    # Display the top words for each topic.\n",
    "    display_topics(lda, tf_feature_names, 10)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    \"\"\"\n",
    "    Helper function to display the top words for each topic in the LDA model.\n",
    "\n",
    "    :param model: the LDA model\n",
    "    :param feature_names: feature names from CounteVectorizer.\n",
    "    :param num_top_words: # of words to display for each topic.\n",
    "    :return: none.\n",
    "    \"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        log.debug(\"Topic %d:\" % (topic_idx))\n",
    "        log.debug(\" \".join([feature_names[i]\n",
    "                            for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function that pre-processes the Tweet text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet_text(tweet_text):\n",
    "    \"\"\"\n",
    "    Helper function performs text pre-processing using regular expressions and other Python functions.\n",
    "\n",
    "    Notes:\n",
    "\n",
    "    TODO - shrink character elongations\n",
    "    TODO - remove non-english tweets\n",
    "    TODO - remove non-company associated tweets\n",
    "    TODO - remove year and time.\n",
    "    TODO - remove cash items?\n",
    "\n",
    "    Resources Used:\n",
    "\n",
    "    https://thispointer.com/python-how-to-convert-a-list-to-string/\n",
    "    http://jonathansoma.com/lede/foundations/classes/pandas%20columns%20and%20functions/apply-a-function-to-every-row-in-a-pandas-dataframe/\n",
    "\n",
    "    :return: the processed Tweet.\n",
    "    \"\"\"\n",
    "\n",
    "    # # Remove \"RT\" tags.\n",
    "    # preprocessed_tweet_text = re.sub(\"rt\", \"\", tweet_text)\n",
    "    #\n",
    "    # # Remove URL's.\n",
    "    # preprocessed_tweet_text = re.sub(\"http[s]?://\\S+\", \"slo_url\", preprocessed_tweet_text)\n",
    "    #\n",
    "    # # Remove Tweet mentions.\n",
    "    # preprocessed_tweet_text = re.sub(\"@\\S+\", \"slo_mention\", preprocessed_tweet_text)\n",
    "    #\n",
    "    # # Remove Tweet hashtags.\n",
    "    # preprocessed_tweet_text = re.sub(\"#\\S+\", \"slo_hashtag\", preprocessed_tweet_text)\n",
    "    #\n",
    "    # # Remove all punctuation.\n",
    "    # preprocessed_tweet_text = preprocessed_tweet_text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove irrelevant words from Tweets.\n",
    "    delete_list = [\"slo_url\", \"slo_mention\", \"word_n\", \"slo_year\", \"slo_cash\", \"woodside\", \"auspol\", \"adani\",\n",
    "                   \"stopadani\",\n",
    "                   \"ausbiz\", \"santos\", \"whitehaven\", \"tinto\", \"fortescue\", \"bhp\", \"adelaide\", \"billiton\", \"csg\",\n",
    "                   \"nswpol\",\n",
    "                   \"nsw\", \"lng\", \"don\", \"rio\", \"pilliga\", \"australia\", \"asx\", \"just\", \"today\", \"great\", \"says\", \"like\",\n",
    "                   \"big\", \"better\", \"rite\", \"would\", \"SCREEN_NAME\", \"mining\", \"former\", \"qldpod\", \"qldpol\", \"qld\", \"wr\",\n",
    "                   \"melbourne\", \"andrew\", \"fuck\", \"spadani\", \"greg\", \"th\", \"australians\", \"http\", \"https\", \"rt\",\n",
    "                   \"goadani\",\n",
    "                   \"co\", \"amp\", \"riotinto\", \"carmichael\", \"abbot\", \"bill shorten\",\n",
    "                   \"slourl\", \"slomention\", \"slohashtag\", \"sloyear\", \"slocash\"]\n",
    "\n",
    "    # Convert series to string.\n",
    "    tweet_string = str(tweet_text)\n",
    "\n",
    "    if debug_preprocess_tweets:\n",
    "        log.debug(\"Tweet text as string:\")\n",
    "        log.debug(tweet_string)\n",
    "        log.debug('\\n')\n",
    "\n",
    "    # Split Tweet into individual words.\n",
    "    words = tweet_string.split()\n",
    "\n",
    "    if debug_preprocess_tweets:\n",
    "        log.debug(\"Tweet text as list:\")\n",
    "        log.debug(words)\n",
    "        log.debug('\\n')\n",
    "\n",
    "    # Check to see if a word is irrelevant or not.\n",
    "    words_relevant = []\n",
    "    for w in words:\n",
    "        if w not in delete_list:\n",
    "            words_relevant.append(w)\n",
    "        else:\n",
    "            if debug_preprocess_tweets:\n",
    "                log.debug(\"Irrelevant word found: \")\n",
    "                log.debug(w)\n",
    "                log.debug('\\n')\n",
    "\n",
    "    if debug_preprocess_tweets:\n",
    "        log.debug(\"List of relevant words in Tweet: \")\n",
    "        log.debug(words_relevant)\n",
    "        log.debug('\\n')\n",
    "\n",
    "    # Convert list back into original Tweet text minus irrelevant words.\n",
    "    tweet_string = ' '.join(words_relevant)\n",
    "    # Convert back to a series object.\n",
    "    tweet_series = pd.Series(tweet_string)\n",
    "\n",
    "    if debug_preprocess_tweets:\n",
    "        log.debug(\"Tweet text with irrelevant words removed: \")\n",
    "        log.debug(tweet_series)\n",
    "        log.debug('\\n')\n",
    "\n",
    "    return tweet_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that pre-processes the Tweet dataset for LDA topic extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_dataset_preprocessor(input_file_path, output_file_path, column_name):\n",
    "    \"\"\"\n",
    "     Function pre-processes specified dataset in preparation for LDA topic extraction.\n",
    "\n",
    "    :param input_file_path: relative filepath from project root directory for location of dataset to process.\n",
    "    :param output_file_path: relative filepath from project root directory for location to save .csv file.\n",
    "    :param column_name: name of the column in the dataset that we are pre-processing.\n",
    "    :return: Nothing. Saves to CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import the dataset.\n",
    "    slo_dataset_cmu = \\\n",
    "        pd.read_csv(str(input_file_path), sep=\",\")\n",
    "\n",
    "    # Shuffle the data randomly.\n",
    "    slo_dataset_cmu = slo_dataset_cmu.reindex(\n",
    "        pd.np.random.permutation(slo_dataset_cmu.index))\n",
    "\n",
    "    # Generate a Pandas dataframe.\n",
    "    slo_dataframe_cmu = pd.DataFrame(slo_dataset_cmu[str(column_name)])\n",
    "\n",
    "    if debug_preprocess_tweets:\n",
    "        # Print shape and column names.\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"The shape of our SLO CMU dataframe:\")\n",
    "        log.debug(slo_dataframe_cmu.shape)\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"The columns of our SLO CMU dataframe:\")\n",
    "        log.debug(slo_dataframe_cmu.head)\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    # # Down-case all text.\n",
    "    # slo_dataframe_cmu['tweet_t'] = slo_dataframe_cmu['tweet_t'].str.lower()\n",
    "\n",
    "    # Pre-process each tweet individually.\n",
    "    slo_dataframe_cmu[str(column_name)] = slo_dataframe_cmu[str(column_name)].apply(preprocess_tweet_text)\n",
    "\n",
    "    # Reindex everything.\n",
    "    slo_dataframe_cmu.index = pd.RangeIndex(len(slo_dataframe_cmu.index))\n",
    "    # slo_dataframe_combined.index = range(len(slo_dataframe_combined.index))\n",
    "\n",
    "    # Save to CSV file.\n",
    "    slo_dataframe_cmu.to_csv(str(output_file_path), sep=',',\n",
    "                             encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function that executes the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Topic 0:\n",
      "DEBUG:root:time pay news foescue good iron ore join national times\n",
      "DEBUG:root:Topic 1:\n",
      "DEBUG:root:adanis coal cou point risk land barnaby joyce deal huge\n",
      "DEBUG:root:Topic 2:\n",
      "DEBUG:root:govt money india coal state approval banks needs free giving\n",
      "DEBUG:root:Topic 3:\n",
      "DEBUG:root:coal new labor mines build taxpayers native come title board\n",
      "DEBUG:root:Topic 4:\n",
      "DEBUG:root:turnbull funding fund basin galilee pm council tell carbon townsville\n",
      "DEBUG:root:Topic 5:\n",
      "DEBUG:root:reef company barrier environmental coal oil cut prices profit corporate\n",
      "DEBUG:root:Topic 6:\n",
      "DEBUG:root:queensland future coalmine business coal renewables investment palaszczuk workers noh\n",
      "DEBUG:root:Topic 7:\n",
      "DEBUG:root:want energy rail line help coal vote clean plan global\n",
      "DEBUG:root:Topic 8:\n",
      "DEBUG:root:climate change does ceo coal community thing politicians debt subsidies\n",
      "DEBUG:root:Topic 9:\n",
      "DEBUG:root:tax lnp greens paid election alp think public labor loan\n",
      "DEBUG:root:Topic 10:\n",
      "DEBUG:root:world doesnt taxpayer biggest shoen pollution stand fossil price coal\n",
      "DEBUG:root:Topic 11:\n",
      "DEBUG:root:slocashn action thanks beach breaking create local planet record im\n",
      "DEBUG:root:Topic 12:\n",
      "DEBUG:root:suppo farmers know way fight did bank protest slomentions gets\n",
      "DEBUG:root:Topic 13:\n",
      "DEBUG:root:project government australian day coal adani campaign canavan really disaster\n",
      "DEBUG:root:Topic 14:\n",
      "DEBUG:root:gas water coal oppose massive work seam forest look protect\n",
      "DEBUG:root:Topic 15:\n",
      "DEBUG:root:people power coal right plans hey solar morning cost opposition\n",
      "DEBUG:root:Topic 16:\n",
      "DEBUG:root:dont minister said years environment thats watch latest political resources\n",
      "DEBUG:root:Topic 17:\n",
      "DEBUG:root:loan make wont indian wants group green federal dam clear\n",
      "DEBUG:root:Topic 18:\n",
      "DEBUG:root:jobs year billion decision week financial owners traditional goes thousands\n",
      "DEBUG:root:Topic 19:\n",
      "DEBUG:root:stop need say going ahead narrabri project industry win let\n",
      "DEBUG:root:The time taken to perform the operation is: \n",
      "DEBUG:root:453.0799798965454\n",
      "DEBUG:root:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main function.  Execute the program.\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    start_time = time.time()\n",
    "    ################################################\n",
    "    \"\"\"\n",
    "    Perform the Tweet preprocessing.\n",
    "    \"\"\"\n",
    "    # tweet_dataset_preprocessor(\"datasets/dataset_20100101-20180510_tok_PROCESSED.csv\",\n",
    "    #                            \"datasets/dataset_20100101-20180510_tok_LDA_PROCESSED.csv\", \"tweet_t\")\n",
    "    # tweet_dataset_preprocessor(\"datasets/tbl_kvlinden_PROCESSED.csv\",\n",
    "    #                            \"datasets/tbl_kvlinden_LDA_PROCESSED.csv\", \"Tweet\")\n",
    "    # tweet_dataset_preprocessor(\"datasets/tbl_training_set_PROCESSED.csv\",\n",
    "    #                            \"datasets/tbl_training_set_LDA_PROCESSED.csv\", \"Tweet\")\n",
    "    \"\"\"\n",
    "    Perform exhaustive grid search.\n",
    "    \"\"\"\n",
    "    # latent_dirichlet_allocation_grid_search()\n",
    "    \"\"\"\n",
    "    Perform the topic extraction.\n",
    "    \"\"\"\n",
    "    latent_dirichlet_allocation_topic_extraction()\n",
    "    ################################################\n",
    "    end_time = time.time()\n",
    "\n",
    "    if debug_pipeline:\n",
    "        log.debug(\"The time taken to perform the operation is: \")\n",
    "        total_time = end_time - start_time\n",
    "        log.debug(str(total_time))\n",
    "        log.debug(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "##### The above shows numerically indexed topics and the top words associated with each topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does it work poorly on Tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "##### Based on Derek Fisher's senior project presentation:\n",
    "\n",
    "1) LDA typically works best when the documents are lengthy (large word count) and written in a formal proper style.\n",
    "\n",
    "2) Tweet text is generally very short in length with a max of around 280 characters.\n",
    "\n",
    "3) Tweet text is generally written very informally style-wise.\n",
    "\n",
    "    i) emojis.\n",
    "    ii) spelling errors.\n",
    "    iii) other grammatical errors.\n",
    "    iv) etc.\n",
    "\n",
    "4) The above makes it difficult for the LDA algorithm to discover any prominent underlying hidden structures.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
