{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Adapted SLO TBL topic classification codebase and Derek Fisher's code for my own LDA topic extraction implemenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources Used:\n",
    "\n",
    "https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158\n",
    "\n",
    "https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n",
    "\n",
    "https://www.investopedia.com/terms/p/posterior-probability.asp\n",
    "\n",
    "https://cs.calvin.edu/courses/cs/x95/videos/2018-2019/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Basics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "##### Basic Concept:\n",
    "\n",
    "Each document described by a distribution of topics.<br>\n",
    "Each topic described by a distribution of words.<br>\n",
    "Typically uses bag-of-words feature representation for documents.<br>\n",
    "Permits the identification of topics within documents and the mapping of documents to associated topics.<br>\n",
    "\n",
    "##### Terms:\n",
    "\n",
    "Observed layer: documents (composites) and words (parts).<br>\n",
    "Hidden (latent) layer: topics (categories).<br>\n",
    "\n",
    "k — Number of topics a document belongs to (a fixed number).<br>\n",
    "\n",
    "V — Size of the vocabulary.<br>\n",
    "\n",
    "M — Number of documents.<br>\n",
    "\n",
    "N — Number of words in each document.<br>\n",
    "\n",
    "w — A word in a document. This is represented as a one hot encoded vector of size V (i.e. V — vocabulary size).<br>\n",
    "\n",
    "w (bold w): represents a document (i.e. vector of “w”s) of N words.<br>\n",
    "\n",
    "D — Corpus, a collection of M documents.<br>\n",
    "\n",
    "z — A topic from a set of k topics. A topic is a distribution words. For example it might be, Animal = (0.3 Cats, 0.4 Dogs, 0 AI, 0.2 Loyal, 0.1 Evil).<br>\n",
    "\n",
    "![lda](lda_model.jpeg)\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "α — Distribution related parameter that governs what the distribution of topics is for all the documents in the corpus looks like.<br>\n",
    "\n",
    "θ — Random matrix where θ(i,j) represents the probability of the i th document to containing the j th topic.<br>\n",
    "\n",
    "η — Distribution related parameter that governs what the distribution of words in each topic looks like.<br>\n",
    "\n",
    "β — A random matrix where β(i,j) represents the probability of i th topic containing the j th word.<br>\n",
    "\n",
    "##### Dirichlet Distribution (example):\n",
    "\n",
    "![dirichlet](dirichlet_distribution.png)\n",
    "\n",
    "1) Large values of α pushes the distribution to the center.<br>\n",
    "2) Small vlues of α pushes the distribution to the edges.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "##### Mathematical equivalent of the above graphical representation of LDA:\n",
    "\n",
    "![mathematical_model](lda_equation.png)\n",
    "\n",
    "##### English Translation:\n",
    "\n",
    "Given a set of M documents with each containing N words and each word generated from a topic \"k\" from a set of K topics, find the joint posterior probability of:\n",
    "\n",
    "θ — A distribution of topics, one for each document,<br>\n",
    "z — N Topics for each document,<br>\n",
    "β — A distribution of words, one for each topic,<br>\n",
    "\n",
    "Given:\n",
    "\n",
    "D — All the data we have (i.e. the corpus),<br>\n",
    "\n",
    "Using the parameters:\n",
    "\n",
    "α — A parameter vector for each document (document — Topic distribution).<br>\n",
    "η — A parameter vector for each topic (topic — word distribution).<br>\n",
    "\n",
    "\n",
    "##### Joint posterior probability: \n",
    "\n",
    "In Bayesian statistics, it is the revised or updated probablity of an event occurring given new information.<br>\n",
    "Calculated by updating the prior probability using Bayes' Theorem.<br>\n",
    "In other words, conditional probability - probability of event A occurring given that event B has occurred.<br>\n",
    "\n",
    "##### Prior probability:\n",
    "\n",
    "In Bayesian statistics, it is the probablity of an event occurring before new information is given.<br>\n",
    "Calculated using Bayes' Theorem.\n",
    "\n",
    "##### Note:\n",
    "\n",
    "Choose # of topics < # of documents to reduce dimensionality for further analysis using other algorithms.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Latent Dirichlet Allocation on SLO Twitter Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Resources Used:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation\n",
    "https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "\n",
    "# Import libraries.\n",
    "import logging as log\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import custom utility functions.\n",
    "import slo_lda_topic_extraction_utility_functions as lda_util\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Miscellaneous parameter adjustments for pandas and python.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "\"\"\"\n",
    "Turn debug log statements for various sections of code on/off.\n",
    "(adjust log level as necessary)\n",
    "\"\"\"\n",
    "log.basicConfig(level=log.INFO)\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "################################################################################################################\n",
    "################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Adjust log verbosity levels as necessary.<br>\n",
    "\n",
    "Set to \"DEBUG\" to view all debug output.<br>\n",
    "Set to \"INFO\" to view useful information on dataframe shape, etc.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and prep the dataset for use in LDA topic extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "\n",
      "INFO:root:The shape of our preprocessed SLO dataframe:\n",
      "INFO:root:(658982, 1)\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:The columns of our preprocessed SLO dataframe:\n",
      "INFO:root:<bound method NDFrame.head of                                                   tweet_t\n",
      "562440  sugar hit for local miners and shares in new y...\n",
      "631505     needs for the coal mine please give generously\n",
      "559563  riding the commodities wave with a nice breako...\n",
      "301050  more than a hundred jobs to go at in sa worker...\n",
      "164203  coal mine gautam adanis brother vinod named in...\n",
      "...                                                   ...\n",
      "504735                                                NaN\n",
      "266067  indigenous owners threaten legal action unless...\n",
      "503361  “ will push this project until no other fundin...\n",
      "425274  no place for coal mines or a massive gasfield ...\n",
      "398365  yeh the 6 was estrela nuno was ��� rochinha is...\n",
      "\n",
      "[658982 rows x 1 columns]>\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:The shape of our preprocessed SLO dataframe with NaN (empty) rows dropped:\n",
      "INFO:root:(653094, 1)\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:The columns of our preprocessed SLO dataframe with NaN (empty) rows dropped:\n",
      "INFO:root:<bound method NDFrame.head of                                                   tweet_t\n",
      "562440  sugar hit for local miners and shares in new y...\n",
      "631505     needs for the coal mine please give generously\n",
      "559563  riding the commodities wave with a nice breako...\n",
      "301050  more than a hundred jobs to go at in sa worker...\n",
      "164203  coal mine gautam adanis brother vinod named in...\n",
      "...                                                   ...\n",
      "572167  alp though pulled ahead in whitsunday alp 51 t...\n",
      "266067  indigenous owners threaten legal action unless...\n",
      "503361  “ will push this project until no other fundin...\n",
      "425274  no place for coal mines or a massive gasfield ...\n",
      "398365  yeh the 6 was estrela nuno was ��� rochinha is...\n",
      "\n",
      "[653094 rows x 1 columns]>\n",
      "INFO:root:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset.\n",
    "tweet_dataset_processed = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/datasets/dataset_20100101-20180510_tok_LDA_PROCESSED.csv\", sep=\",\")\n",
    "\n",
    "# Reindex and shuffle the data randomly.\n",
    "tweet_dataset_processed = tweet_dataset_processed.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_dataframe_processed = pd.DataFrame(tweet_dataset_processed)\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(\"\\n\")\n",
    "log.info(\"The shape of our preprocessed SLO dataframe:\")\n",
    "log.info(tweet_dataframe_processed.shape)\n",
    "log.info(\"\\n\")\n",
    "log.info(\"The columns of our preprocessed SLO dataframe:\")\n",
    "log.info(tweet_dataframe_processed.head)\n",
    "log.info(\"\\n\")\n",
    "\n",
    "# Drop any NaN or empty Tweet rows in dataframe (or else CountVectorizer will blow up).\n",
    "tweet_dataframe_processed = tweet_dataframe_processed.dropna()\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(\"\\n\")\n",
    "log.info(\"The shape of our preprocessed SLO dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(tweet_dataframe_processed.shape)\n",
    "log.info(\"\\n\")\n",
    "log.info(\"The columns of our preprocessed SLO dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(tweet_dataframe_processed.head)\n",
    "log.info(\"\\n\")\n",
    "\n",
    "# Reindex everything.\n",
    "tweet_dataframe_processed.index = pd.RangeIndex(len(tweet_dataframe_processed.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_dataframe_processed_column_names = ['Tweet']\n",
    "\n",
    "# Rename column in dataframe.\n",
    "tweet_dataframe_processed.columns = tweet_dataframe_processed_column_names\n",
    "\n",
    "# Create input feature.\n",
    "selected_features = tweet_dataframe_processed[tweet_dataframe_processed_column_names]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "# Check what we are using as inputs.\n",
    "log.debug(\"\\n\")\n",
    "log.debug(\"The Tweets in our input feature:\")\n",
    "log.debug(processed_features['Tweet'])\n",
    "log.debug(\"\\n\")\n",
    "\n",
    "# Create feature set.\n",
    "slo_feature_set = processed_features['Tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "The above log.INFO messages depict the shape and contents of the preprocessed dataframe before and after dropping any rows that are just \"NaN\", indicating the Tweet was full of irrelevant words.<br>\n",
    "\n",
    "The rest of the code simply imports our dataset, puts it into a Pandas dataframe, drops any NaN rows to avoid blowing up CountVectorizer, re-assigns the column name to \"Tweet\", and assigns a name to the input feature we will use for LDA topic extraction.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that performs the topic extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_dirichlet_allocation_topic_extraction():\n",
    "    \"\"\"\n",
    "    Function performs topic extraction on Tweets using Scikit-Learn LDA model.\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model.\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(slo_feature_set)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    # Run LDA.\n",
    "    lda = LatentDirichletAllocation(n_topics=20, max_iter=5, learning_method='online', learning_offset=50.,\n",
    "                                    random_state=0).fit(tf)\n",
    "\n",
    "    # Display the top words for each topic.\n",
    "    lda_util.display_topics(lda, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "The above is the Scikit-Learn implementation of LDA.  We use the CountVectorizer class to vectorize our input feature and then fit the LDA model to our data.<br>\n",
    "\n",
    "We call a utility function to display the top words associated with each topic.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main executes the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "################################################\n",
    "\"\"\"\n",
    "Perform the Twitter dataset preprocessing.\n",
    "\"\"\"\n",
    "# lda_util.tweet_dataset_preprocessor(\"datasets/dataset_20100101-20180510_tok_PROCESSED.csv\",\n",
    "#                                     \"datasets/dataset_20100101-20180510_tok_LDA_PROCESSED2.csv\", \"tweet_t\")\n",
    "\"\"\"\n",
    "Perform exhaustive grid search.\n",
    "\"\"\"\n",
    "# latent_dirichlet_allocation_grid_search()\n",
    "\"\"\"\n",
    "Perform the topic extraction.\n",
    "\"\"\"\n",
    "latent_dirichlet_allocation_topic_extraction()\n",
    "################################################\n",
    "end_time = time.time()\n",
    "\n",
    "log.info(\"The time taken to perform the operation is: \")\n",
    "total_time = end_time - start_time\n",
    "log.info(str(total_time))\n",
    "log.info(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Tweet preprocessing is done via a custom library imported as \"lda_util\" from \"slo_lda_topic_extraction_utility_functions.py\"<br>\n",
    "\n",
    "The function to perform exhaustive grid search isn't currently used.  We will use it in the future once we fully understand LDA, its associated hyper parameters, and how to tune for improved results.<br>\n",
    "\n",
    "It takes around 450 seconds or so to finish LDA topic extraction per execution, so it is not a particularly fast process.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from a different execution of LDA topic extraction on our dataset:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Topic 0:\n",
    "loan rail lnp federal line fund think naif use story\n",
    "Topic 1:\n",
    "need group community carbon country stand high week end wrong\n",
    "Topic 2:\n",
    "tax breaking paid industry ceo green china financial hey licence\n",
    "Topic 3:\n",
    "government new money day environmental barnaby joyce taxpayers plans giving\n",
    "Topic 4:\n",
    "news funding foescue shoen help thanks pm beach bank cut\n",
    "Topic 5:\n",
    "suppo going world wont greens work vote let companies dollars\n",
    "Topic 6:\n",
    "time farmers year look global fossil profit video police best\n",
    "Topic 7:\n",
    "people say campaign free indian giant local australias corruption prices\n",
    "Topic 8:\n",
    "gas coal public slocashn project deal does adani risk biggest\n",
    "Topic 9:\n",
    "action fight state did join decision protest doesnt getting read\n",
    "Topic 10:\n",
    "climate project coal change repo environment price shares doing lost\n",
    "Topic 11:\n",
    "minister election build years native title canavan away planet owners\n",
    "Topic 12:\n",
    "reef turnbull cou barrier coal ahead know adanis wants approval\n",
    "Topic 13:\n",
    "india company pay coal alp business right oil oppose win\n",
    "Topic 14:\n",
    "coal good power make mines iron ore new po protect\n",
    "Topic 15:\n",
    "jobs australian needs create thats billion tell got investment workers\n",
    "Topic 16:\n",
    "adanis energy future coal land taxpayer said clean destroy gov\n",
    "Topic 17:\n",
    "want govt dont point coal massive politicians infrastructure protesters subsidies\n",
    "Topic 18:\n",
    "stop water queensland narrabri coal disaster corporate unlimited live tonight\n",
    "Topic 19:\n",
    "labor coalmine coal galilee way huge basin job pollution townsville\n",
    "INFO:root:The time taken to perform the operation is: \n",
    "INFO:root:452.49793434143066\n",
    "INFO:root:\n",
    "\n",
    "\n",
    "Process finished with exit code 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive grid search for Scikit-Learn LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_dirichlet_allocation_grid_search():\n",
    "    \"\"\"\n",
    "    Function performs exhaustive grid search for Scikit-Learn LDA model.\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # Construct the pipeline.\n",
    "    latent_dirichlet_allocation_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')),\n",
    "        ('clf', LatentDirichletAllocation()),\n",
    "    ])\n",
    "\n",
    "    # What parameters do we search for?\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "        'clf__n_components': [1, 5, 10, 15],\n",
    "        'clf__doc_topic_prior': [None],\n",
    "        'clf__topic_word_prior': [None],\n",
    "        'clf__learning_method': ['batch', 'online'],\n",
    "        'clf__learning_decay': [0.5, 0.7, 0.9],\n",
    "        'clf__learning_offset': [5, 10, 15],\n",
    "        'clf__max_iter': [5, 10, 15],\n",
    "        'clf__batch_size': [64, 128, 256],\n",
    "        'clf__evaluate_every': [0],\n",
    "        'clf__total_samples': [1e4, 1e6, 1e8],\n",
    "        'clf__perp_tol': [1e-1, 1e-2, 1e-3],\n",
    "        'clf__mean_change_tol': [1e-1, 1e-3, 1e-5],\n",
    "        'clf__max_doc_update_iter': [50, 100, 150],\n",
    "        'clf__n_jobs': [-1],\n",
    "        'clf__verbose': [0],\n",
    "        'clf__random_state': [None],\n",
    "    }\n",
    "\n",
    "    # Perform the grid search.\n",
    "    latent_dirichlet_allocation_clf = GridSearchCV(latent_dirichlet_allocation_clf, parameters, cv=5, iid=False,\n",
    "                                                   n_jobs=-1)\n",
    "    latent_dirichlet_allocation_clf.fit(slo_feature_set)\n",
    "\n",
    "    # View all the information stored in the model after training it.\n",
    "    classifier_results = pd.DataFrame(latent_dirichlet_allocation_clf.cv_results_)\n",
    "    log.debug(\"The shape of the Latent Dirichlet Allocation model's result data structure is:\")\n",
    "    log.debug(classifier_results.shape)\n",
    "    log.debug(\n",
    "        \"The contents of the Latent Dirichlet Allocation model's result data structure is:\")\n",
    "    log.debug(classifier_results.head())\n",
    "\n",
    "    # Display the optimal parameters.\n",
    "    log.info(\"The optimal parameters found for the Latent Dirichlet Allocation is:\")\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        log.info(\"%s: %r\" % (param_name, latent_dirichlet_allocation_clf.best_params_[param_name]))\n",
    "    log.info(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "We use Scikit-Learn's Pipeline Class to construct a pipeline consisting of the CounteVectorizer and LatentDirichletAllocation classes.<br>\n",
    "\n",
    "The \"parameters\" dictionary determine all the possible combinations of hyper parameters we will test in order to find the optimal hyper parameters for the Scikit-Learn LDA model.<br>\n",
    "\n",
    "The grid search is performed by fitting on the data we wish to use for topic extraction.<br>\n",
    "\n",
    "The optimal hyper parameters are displayed via \"log.info\" messages so the log verbosity level must be set appropriately to view them.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does it work poorly on Tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "##### Based on Derek Fisher's senior project presentation:\n",
    "\n",
    "1) LDA typically works best when the documents are lengthy (large word count) and written in a formal proper style.\n",
    "\n",
    "2) Tweet text is generally very short in length with a max of around 280 characters.\n",
    "\n",
    "3) Tweet text is generally written very informally style-wise.\n",
    "\n",
    "    i) emojis.\n",
    "    ii) spelling errors.\n",
    "    iii) other grammatical errors.\n",
    "    iv) etc.\n",
    "\n",
    "4) The above makes it difficult for the LDA algorithm to discover any prominent underlying hidden structures.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "## Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Refer to the below URL link to the utility functions used above for data preprocessing and LDA topic extraction:\n",
    "\n",
    "Placeholder.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
