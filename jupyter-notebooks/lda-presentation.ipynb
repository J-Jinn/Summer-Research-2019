{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Tentatively finished.  Might update the codebase given progress on reproducing Derek Fisher's results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources Used:\n",
    "\n",
    "https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158\n",
    "\n",
    "https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n",
    "\n",
    "https://www.investopedia.com/terms/p/posterior-probability.asp\n",
    "\n",
    "https://cs.calvin.edu/courses/cs/x95/videos/2018-2019/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Basics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "##### Basic Concept:\n",
    "\n",
    "Each document described by a distribution of topics.<br>\n",
    "Each topic described by a distribution of words.<br>\n",
    "Typically uses bag-of-words feature representation for documents.<br>\n",
    "Permits the identification of topics within documents and the mapping of documents to associated topics.<br>\n",
    "\n",
    "##### Terms:\n",
    "\n",
    "Observed layer: documents (composites) and words (parts).<br>\n",
    "Hidden (latent) layer: topics (categories).<br>\n",
    "\n",
    "k — Number of topics a document belongs to (a fixed number).<br>\n",
    "\n",
    "V — Size of the vocabulary.<br>\n",
    "\n",
    "M — Number of documents.<br>\n",
    "\n",
    "N — Number of words in each document.<br>\n",
    "\n",
    "w — A word in a document. This is represented as a one hot encoded vector of size V (i.e. V — vocabulary size).<br>\n",
    "\n",
    "w (bold w): represents a document (i.e. vector of “w”s) of N words.<br>\n",
    "\n",
    "D — Corpus, a collection of M documents.<br>\n",
    "\n",
    "z — A topic from a set of k topics. A topic is a distribution words. For example it might be, Animal = (0.3 Cats, 0.4 Dogs, 0 AI, 0.2 Loyal, 0.1 Evil).<br>\n",
    "\n",
    "![lda](lda_model.jpeg)\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "α — Distribution related parameter that governs what the distribution of topics is for all the documents in the corpus looks like.<br>\n",
    "\n",
    "θ — Random matrix where θ(i,j) represents the probability of the i th document to containing the j th topic.<br>\n",
    "\n",
    "η — Distribution related parameter that governs what the distribution of words in each topic looks like.<br>\n",
    "\n",
    "β — A random matrix where β(i,j) represents the probability of i th topic containing the j th word.<br>\n",
    "\n",
    "##### Dirichlet Distribution (example):\n",
    "\n",
    "![dirichlet](dirichlet_distribution.png)\n",
    "\n",
    "1) Large values of α pushes the distribution to the center.<br>\n",
    "2) Small vlues of α pushes the distribution to the edges.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "##### Mathematical equivalent of the above graphical representation of LDA:\n",
    "\n",
    "![mathematical_model](lda_equation.png)\n",
    "\n",
    "##### English Translation:\n",
    "\n",
    "Given a set of M documents with each containing N words and each word generated from a topic \"k\" from a set of K topics, find the joint posterior probability of:\n",
    "\n",
    "θ — A distribution of topics, one for each document,<br>\n",
    "z — N Topics for each document,<br>\n",
    "β — A distribution of words, one for each topic,<br>\n",
    "\n",
    "Given:\n",
    "\n",
    "D — All the data we have (i.e. the corups),<br>\n",
    "\n",
    "Using the parameters:\n",
    "\n",
    "α — A parameter vector for each document (document — Topic distribution).<br>\n",
    "η — A parameter vector for each topic (topic — word distribution).<br>\n",
    "\n",
    "\n",
    "##### Joint posterior probability: \n",
    "\n",
    "In Bayesian statistics, it is the revised or updated probablity of an event occurring given new information.<br>\n",
    "Calculated by updating the prior probability using Bayes' Theorem.<br>\n",
    "In other words, conditional probability - probability of event A occurring given that event B has occurred.<br>\n",
    "\n",
    "##### Prior probability:\n",
    "\n",
    "In Bayesian statistics, it is the probablity of an event occurring before new information is given.<br>\n",
    "Calculated using Bayes' Theorem.\n",
    "\n",
    "##### Note:\n",
    "\n",
    "Choose # of topics < # of documents to reduce dimensionality for further analysis using other algorithms.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of using a LDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset to be used in the LDA model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SLO Topic Modeling\n",
    "Advisor: Professor VanderLinden\n",
    "Name: Joseph Jinn\n",
    "Date: 5-29-19\n",
    "\n",
    "LDA - Latent Dirichlet Allocation\n",
    "\n",
    "###########################################################\n",
    "Notes:\n",
    "\n",
    "LDA can only use raw term counts (CANNOT use tfidf transformer)\n",
    "\n",
    "###########################################################\n",
    "Resources Used:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation\n",
    "\n",
    "https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "\n",
    "import logging as log\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Note: Need to set level AND turn on debug variables in order to see all debug output.\n",
    "log.basicConfig(level=log.DEBUG)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Miscellaneous parameter adjustments for pandas and python.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "\"\"\"\n",
    "Turn debug log statements for various sections of code on/off.\n",
    "\"\"\"\n",
    "# Debug the GridSearch functions for each Classifier.\n",
    "debug_pipeline = True\n",
    "# Debug the initial dataset import and feature/target set creation.\n",
    "debug_preprocess_tweets = False\n",
    "# Debug create_training_and_test_set() function.\n",
    "debug_train_test_set_creation = False\n",
    "\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "\n",
    "# Import the datasets.\n",
    "tweet_dataset_processed1 = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/datasets/tbl_kvlinden_PROCESSED.csv\", sep=\",\")\n",
    "\n",
    "tweet_dataset_processed2 = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/datasets/tbl_training_set_PROCESSED.csv\", sep=\",\")\n",
    "\n",
    "# Reindex and shuffle the data randomly.\n",
    "tweet_dataset_processed1 = tweet_dataset_processed1.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed1.index))\n",
    "\n",
    "tweet_dataset_processed2 = tweet_dataset_processed2.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed2.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_dataframe_processed1 = pd.DataFrame(tweet_dataset_processed1)\n",
    "tweet_dataframe_processed2 = pd.DataFrame(tweet_dataset_processed2)\n",
    "\n",
    "if debug_preprocess_tweets:\n",
    "    # Print shape and column names.\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The shape of our SLO dataframe 1:\")\n",
    "    log.debug(tweet_dataframe_processed1.shape)\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The columns of our SLO dataframe 1:\")\n",
    "    log.debug(tweet_dataframe_processed1.head)\n",
    "    log.debug(\"\\n\")\n",
    "    # Print shape and column names.\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The shape of our SLO dataframe 2:\")\n",
    "    log.debug(tweet_dataframe_processed2.shape)\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The columns of our SLO dataframe 2:\")\n",
    "    log.debug(tweet_dataframe_processed2.head)\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "# Concatenate the individual datasets together.\n",
    "frames = [tweet_dataframe_processed1, tweet_dataframe_processed2]\n",
    "slo_dataframe_combined = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Reindex everything.\n",
    "slo_dataframe_combined.index = pd.RangeIndex(len(slo_dataframe_combined.index))\n",
    "# slo_dataframe_combined.index = range(len(slo_dataframe_combined.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_dataframe_processed_column_names = ['Tweet', 'SLO']\n",
    "\n",
    "# Create input features.\n",
    "selected_features = slo_dataframe_combined[tweet_dataframe_processed_column_names]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "if debug_preprocess_tweets:\n",
    "    # Check what we are using as inputs.\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The Tweets in our input feature:\")\n",
    "    log.debug(processed_features['Tweet'])\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"SLO TBL topic classification label for each Tweet:\")\n",
    "    log.debug(processed_features['SLO'])\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "# Create feature set and target sets.\n",
    "slo_feature_set = processed_features['Tweet']\n",
    "slo_target_set = processed_features['SLO']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "def create_training_and_test_set():\n",
    "    \"\"\"\n",
    "    This functions splits the feature and target set into training and test sets for each set.\n",
    "\n",
    "    Note: We use this to generate a randomized training and target set in order to average our results over\n",
    "    n iterations.\n",
    "\n",
    "    random_state = rng (where rng = random number seed generator)\n",
    "\n",
    "    :return: Nothing.  Global variables are established.\n",
    "    \"\"\"\n",
    "    global tweet_train, tweet_test, target_train, target_test, target_train_encoded, target_test_encoded\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    import random\n",
    "    rng = random.randint(1, 1000000)\n",
    "    # Split feature and target set into training and test sets for each set.\n",
    "    tweet_train, tweet_test, target_train, target_test = train_test_split(slo_feature_set, slo_target_set,\n",
    "                                                                          test_size=0.33,\n",
    "                                                                          random_state=rng)\n",
    "\n",
    "    if debug_train_test_set_creation:\n",
    "        log.debug(\"Shape of tweet training set:\")\n",
    "        log.debug(tweet_train.data.shape)\n",
    "        log.debug(\"Shape of tweet test set:\")\n",
    "        log.debug(tweet_test.data.shape)\n",
    "        log.debug(\"Shape of target training set:\")\n",
    "        log.debug(target_train.data.shape)\n",
    "        log.debug(\"Shape of target test set:\")\n",
    "        log.debug(target_test.data.shape)\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    # Use Sci-kit learn to encode labels into integer values - one assigned integer value per class.\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    target_label_encoder = preprocessing.LabelEncoder()\n",
    "    target_train_encoded = target_label_encoder.fit_transform(target_train)\n",
    "    target_test_encoded = target_label_encoder.fit_transform(target_test)\n",
    "\n",
    "    target_train_decoded = target_label_encoder.inverse_transform(target_train_encoded)\n",
    "    target_test_decoded = target_label_encoder.inverse_transform(target_test_encoded)\n",
    "\n",
    "    if debug_train_test_set_creation:\n",
    "        log.debug(\"Encoded target training labels:\")\n",
    "        log.debug(target_train_encoded)\n",
    "        log.debug(\"Decoded target training labels:\")\n",
    "        log.debug(target_train_decoded)\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"Encoded target test labels:\")\n",
    "        log.debug(target_test_encoded)\n",
    "        log.debug(\"Decoded target test labels:\")\n",
    "        log.debug(target_test_decoded)\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    # return [tweet_train, tweet_test, target_train, target_test, target_train_encoded, target_test_encoded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive grid search for Scikit-Learn LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "\n",
    "def latent_dirichlet_allocation_grid_search():\n",
    "    \"\"\"\n",
    "    Function performs exhaustive grid search for LDA.\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "    # Create randomized training and test set using our dataset.\n",
    "    create_training_and_test_set()\n",
    "\n",
    "    # Construct the pipeline.\n",
    "    latent_dirichlet_allocation_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')),\n",
    "        ('clf', LatentDirichletAllocation()),\n",
    "    ])\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # What parameters do we search for?\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "        'clf__n_components': [1, 5, 10, 15],\n",
    "        'clf__doc_topic_prior': [None],\n",
    "        'clf__topic_word_prior': [None],\n",
    "        'clf__learning_method': ['batch', 'online'],\n",
    "        'clf__learning_decay': [0.5, 0.7, 0.9],\n",
    "        'clf__learning_offset': [5, 10, 15],\n",
    "        'clf__max_iter': [5, 10, 15],\n",
    "        'clf__batch_size': [64, 128, 256],\n",
    "        'clf__evaluate_every': [0],\n",
    "        'clf__total_samples': [1e4, 1e6, 1e8],\n",
    "        'clf__perp_tol': [1e-1, 1e-2, 1e-3],\n",
    "        'clf__mean_change_tol': [1e-1, 1e-3, 1e-5],\n",
    "        'clf__max_doc_update_iter': [50, 100, 150],\n",
    "        'clf__n_jobs': [-1],\n",
    "        'clf__verbose': [0],\n",
    "        'clf__random_state': [None],\n",
    "    }\n",
    "\n",
    "    # Perform the grid search.\n",
    "    latent_dirichlet_allocation_clf = GridSearchCV(latent_dirichlet_allocation_clf, parameters, cv=5, iid=False,\n",
    "                                                   n_jobs=-1)\n",
    "    latent_dirichlet_allocation_clf.fit(tweet_train)\n",
    "\n",
    "    if debug_pipeline:\n",
    "        # View all the information stored in the model after training it.\n",
    "        classifier_results = pd.DataFrame(latent_dirichlet_allocation_clf.cv_results_)\n",
    "        log.debug(\"The shape of the Latent Dirichlet Allocation model's result data structure is:\")\n",
    "        log.debug(classifier_results.shape)\n",
    "        log.debug(\n",
    "            \"The contents of the Latent Dirichlet Allocation model's result data structure is:\")\n",
    "        log.debug(classifier_results.head())\n",
    "\n",
    "    # Display the optimal parameters.\n",
    "    log.debug(\"The optimal parameters found for the Latent Dirichlet Allocation is:\")\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        log.debug(\"%s: %r\" % (param_name, latent_dirichlet_allocation_clf.best_params_[param_name]))\n",
    "    log.debug(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that performs the topic extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "\n",
    "def latent_dirichlet_allocation_topic_extraction():\n",
    "    \"\"\"\n",
    "    Function performs topic extraction on Tweets using LDA.\n",
    "\n",
    "    :return: none.\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "    # Create randomized training and test set using our dataset.\n",
    "    create_training_and_test_set()\n",
    "\n",
    "    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model.\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(tweet_train)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    # Run LDA.\n",
    "    lda = LatentDirichletAllocation(n_topics=20, max_iter=5, learning_method='online', learning_offset=50.,\n",
    "                                    random_state=0).fit(tf)\n",
    "\n",
    "    # Display the top words for each topic.\n",
    "    display_topics(lda, tf_feature_names, 10)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    \"\"\"\n",
    "    Helper function to display the top words for each topic in the LDA model.\n",
    "\n",
    "    :param model: the LDA model\n",
    "    :param feature_names: feature names from CounteVectorizer\n",
    "    :param num_top_words: # of words to display for each topic.\n",
    "    :return: none.\n",
    "    \"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function that executes the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:The time taken to perform LDA is: \n",
      "DEBUG:root:0.5928463935852051\n",
      "DEBUG:root:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "slomention massive reapproved adani slohashtag work minister enviro approved says\n",
      "Topic 1:\n",
      "slohashtag slourl slomention future shenhua national hinge ffs ruled offences\n",
      "Topic 2:\n",
      "slomention slohashtag slourl saviour said intends adani donations time structural\n",
      "Topic 3:\n",
      "slohashtag slomention slourl coal adani https bhp amp just labor\n",
      "Topic 4:\n",
      "renewables slourl sa coalmine spruiker begin thoughts second canary rollback\n",
      "Topic 5:\n",
      "slohashtag transition stop adani said risks trojan saskatchewa whats real\n",
      "Topic 6:\n",
      "slomention slohashtag adani coal amp slourl change climate link definite\n",
      "Topic 7:\n",
      "slohashtag thats suit turnbull rules right pressure person nemo funding\n",
      "Topic 8:\n",
      "link definite insanity coal protect reason slourl ffs won climate\n",
      "Topic 9:\n",
      "07 major knows https solar supply approved resources open activists\n",
      "Topic 10:\n",
      "santoss sentence did group wake qld fracking surveyed clear project\n",
      "Topic 11:\n",
      "slohashtag check real background offences assess foreign fails corporate ignores\n",
      "Topic 12:\n",
      "liverpool progress prices saskatchewa hurry 07 ramesh delay need doing\n",
      "Topic 13:\n",
      "slohashtag slomention solar wrong 07 cou rio destroy hinge land\n",
      "Topic 14:\n",
      "resources labor liverpool want really slomention begin people australians risk\n",
      "Topic 15:\n",
      "slohashtag industry country fails reefkilling sacrificing link use water narrabri\n",
      "Topic 16:\n",
      "slourl federal dead farming shenhua jairam talking samarco forum moodys\n",
      "Topic 17:\n",
      "slomention coal environment minister finally recognised projects approving reefkilling years\n",
      "Topic 18:\n",
      "slomention adani central george queensland coal prosperity best innovative economic\n",
      "Topic 19:\n",
      "slomention target fresh australians environmental official owners shenhua clear news\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main function.  Execute the program.\n",
    "\"\"\"\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    ################################################\n",
    "    \"\"\"\n",
    "    Scikit-Learn NMF-LDA example.\n",
    "    \"\"\"\n",
    "    # topic_extraction_nmf_lda_example()\n",
    "\n",
    "    \"\"\"\n",
    "    Perform exhaustive grid search.\n",
    "    \"\"\"\n",
    "    # latent_dirichlet_allocation_grid_search()\n",
    "\n",
    "    \"\"\"\n",
    "    Perform the topic extraction.\n",
    "    \"\"\"\n",
    "    latent_dirichlet_allocation_topic_extraction()\n",
    "\n",
    "    ################################################\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    if debug_pipeline:\n",
    "        log.debug(\"The time taken to perform LDA is: \")\n",
    "        total_time = end_time - start_time\n",
    "        log.debug(str(total_time))\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "##### The above shows numerically indexed topics and the top words associated with each topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does it work poorly on Tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "##### Based on Derek Fisher's senior project presentation:\n",
    "\n",
    "1) LDA typically works best when the documents are lengthy (large word count) and written in a formal proper style.\n",
    "\n",
    "2) Tweet text is generally very short in length with a max of around 280 characters.\n",
    "\n",
    "3) Tweet text is generally written very informally style-wise.\n",
    "\n",
    "    i) emojis.\n",
    "    ii) spelling errors.\n",
    "    iii) other grammatical errors.\n",
    "    iv) etc.\n",
    "\n",
    "4) The above makes it difficult for the LDA algorithm to discover any prominent underlying hidden structures.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
