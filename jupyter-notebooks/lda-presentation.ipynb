{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Concept behind LDA's:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "</p>Latent Dirichlet Allocation is a probabilistic method of text analysis for topic modeling.  This method identifies the topics that exists within a set of a documents and maps those documents to their associated topics.  The process typically uses a bag-of-words feature representation for the documents of interest.  In Lda's, each document is described by a distribution of topics and each topic is described by a distribution of words.  There are two primary components to LDA's.  The observed layer are the documents (also called composites) and the words that comprise those documents (the parts).  The hidden (or latent) layer consists of the topics (also called categories) as well as the various variables utilized by the algorithm.  The output of the algorithm is a list of the topics associated with the entire set of documents and the top words associated with each topic.  These topics are indexed values assigned integer values to which they are later assigned English descriptors to describe those topics.</p>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plate Notation for the LDA Algorithm (from \"Intuitive Guide to Latent Dirichlet Allocation\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "</p>The plate notation below represents the algorithm in a graphical format. α is the parameter for the Dirichlet distribution prior that influences the topic-document distribution described by θ. η is the parameter for the Dirichlet distribution prior that influences the word-topic distribution described by β.  While shown as a constant value below, alpha and eta are actually 1-d vectors with length determined by the set of topics (k) that we specify.</p><br><br>\n",
    "\n",
    "</p>The largest plate surrounds all the variables related to a single document in the set of documents (M) that comprise the corpus of interest.  The plate indicates that the variables contained within are repeated M times, once for each document, which also represents a for loop in the pseudocode for the algorithm.</p><br><br>\n",
    "\n",
    "</p>The smaller plate within the largest plate surround all the variables related to a single word within a single document.  The plate indicates that the variables contained within are repeated N times, once for each word for the N words that comprise each of the M documents.  This smaller plate represents a nested for loop within the outer for loop represented by the largest plate.</p><br><br>\n",
    "\n",
    "</p>Within the smaller plate, the variable \"z\" represents a single topic chosen from the topic distribution which represents the distribution of words that belong to that topic.  The variable \"w\" represents the the actual word itself.</p><br><br>\n",
    "    \n",
    "</p>\"w\" is shaded because it is a observed variable belonging to the observed layer.  All other variables are unshaded as they belong to the latent (hidden) layer that cannot be directly observed.</p><br><br>\n",
    "\n",
    "</p>The directed edges between each circle representing each variable indicates dependencies between the variables.  The variable at the head of the edges depend on the variable at the tail of the edges.</p><br><br>\n",
    "\n",
    "</p>The topmost plate surrounds the β word-topic distribution and indicates a for loop where we determine the word-topic distribution for each topic in the set of topics (k).  This is similar to the largest plate surrounding the θ topic-document distribution where there is a for loop that determines the topic-document distribution for each document in the set of documents (M). </p><br><br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lda](lda-presentation-images/lda_model.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Key Terminology and Notation (from \"Intuitive Guide to Latent Dirichlet Allocation\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "k — Number of topics a document belongs to (a fixed number).<br>\n",
    "\n",
    "V — Size of the vocabulary.<br>\n",
    "\n",
    "M — Number of documents.<br>\n",
    "\n",
    "N — Number of words in each document.<br>\n",
    "\n",
    "w — A word in a document. This is represented as a one hot encoded vector of size V (i.e. V — vocabulary size).<br>\n",
    "\n",
    "z — A topic from a set of k topics. A topic is a distribution words. For example it might be, Animal = (0.3 Cats, 0.4 Dogs, 0 AI, 0.2 Loyal, 0.1 Evil).<br>\n",
    "\n",
    "α — Distribution related parameter that governs what the distribution of topics is for all the documents in the corpus looks like.<br>\n",
    "\n",
    "θ — Random matrix where θ(i,j) represents the probability of the i th document to containing the j th topic.<br>\n",
    "\n",
    "η — Distribution related parameter that governs what the distribution of words in each topic looks like.<br>\n",
    "\n",
    "β — A random matrix where β(i,j) represents the probability of i th topic containing the j th word.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical formula for the LDA algorithm (from \"Intuitive Guide to Latent Dirichlet Allocation\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "![mathematical_model](lda-presentation-images/lda_equation.png)\n",
    "\n",
    "##### English Layman's Translation:\n",
    "\n",
    "Given a set of M documents each containing N words with each word generated by a topic from a set of k topics, find the joint posterior probability of:\n",
    "\n",
    "θ — A distribution of topics, one for each document,<br>\n",
    "z — A single topic from the N words for each document,<br>\n",
    "β — A distribution of words, one for each topic,<br>\n",
    "\n",
    "Given:\n",
    "\n",
    "D — All the data we have (i.e. the corpus),<br>\n",
    "\n",
    "Using the parameters:\n",
    "\n",
    "α — A parameter vector for each document (document—topic distribution).<br>\n",
    "η — A parameter vector for each topic (topic—word distribution).<br>\n",
    "\n",
    "\n",
    "##### Joint posterior probability: \n",
    "\n",
    "The revised or updated probablity of an event occurring given new information.<br>\n",
    "Calculated by updating the prior probability using Bayes' Theorem.<br>\n",
    "In other words, conditional probability - probability of event A occurring given that event B has occurred.<br>\n",
    "\n",
    "##### Prior probability:\n",
    "\n",
    "The probablity of an event occurring before new information is given.<br>\n",
    "Calculated using Bayes' Theorem.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "##### Dirichlet Distribution (example):\n",
    "\n",
    "![dirichlet](lda-presentation-images/dirichlet_distribution.png)\n",
    "\n",
    "1) Large values of α pushes the distribution to the center.<br>\n",
    "2) Small values of α pushes the distribution to the edges.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "</p>The graphs above visualize Dirichlet Distributions using 3 topics (k = 3).  The values for α (alpha) and η (eta) influence the shape of the graphs.  By shape, we mean the shape of the probability density function that determines the θ and β distributions .  In this example, the graph is 3-d because we have k = 3 topics.  As k increases, the graphs would become k-dimensional Dirichlet Distribution graphs.</p><br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode for the LDA algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Assign topic (z) to each word (w) in each document (d) (randomly or based on some probabilistic distribution)\n",
    "\n",
    "while(NOT exhausted time constraints)\n",
    "\n",
    "    for each document (d)\n",
    "        for each word (w)\n",
    "            for each topic(z)\n",
    "\n",
    "                Compute Probability(topic (z) | document (d))\n",
    "                Compute Probability(word (w) | topic (z))\n",
    "\n",
    "            Assign new topic (z') to word (W) in document (d) (based on selection using computed probabilities).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "</p>The algorithm for Latent Dirichlet Allocation iteratively assigns a topic to each word in each document based on the computed conditional probabilities of a topic belonging to a document and a word belonging to a topic.  This is repeated until the allocated compute time is exhausted.</p><br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simplified Latent Dirichlet Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics for our example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Topics (k=2) |\n",
    "|--------------|\n",
    "| Topic 1      |\n",
    "| Topic 2      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "It should be noted that the topics in a LDA model are actually just indexed (integer) values from 0-Z and not actually described by any sort of noun, verb, etc.  We later assign \"food\" and \"animals\" as the descriptors for the two topics as we see that the top N words for each indexed topic are strongly associated with those descriptors.  The # of topics and # of top words for each topic are determined by hyper parameter settings set by the user.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial topic assignment for each word in each document:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    Documents (M = 5, N = 3)    |   Word 1   |  Word 2  |  Word 3  |   |\n",
    "|:------------------------------:|:----------:|:--------:|:--------:|:-:|\n",
    "| Doc 1 Word Topic Assignment--> |      1     |     2    |     1    |   |\n",
    "|           Document 1           |     eat    | broccoli |  banana  |   |\n",
    "| Doc 2 Word Topic Assignment--> |      2     |     1    |     2    |   |\n",
    "|           Document 2           |   banana   |  spinach |   lunch  |   |\n",
    "| Doc 3 Word Topic Assignment--> |      1     |     2    |     1    |   |\n",
    "|           Document 3           | chinchilla |  kitten  |   cute   |   |\n",
    "| Doc 4 Word Topic Assignment--> |      2     |     1    |     2    |   |\n",
    "|           Document 4           |   sister   |  kitten  |   today  |   |\n",
    "| Doc 5 Word Topic Assignment--> |      1     |     2    |     1    |   |\n",
    "|           Document 5           |   hamster  |    eat   | broccoli |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "The above is step 1 in the LDA algorithm pseudocode.  For the purposes of this example, we simply randomly assign a topic to each word for each document rather than use a probabilistic distribution.<br>\n",
    "\n",
    "M = 5 indicates that we have five documents total.<br>\n",
    "N =3 indicates that we have 3 word per document.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The list of unique words in our vocabulary (V):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Words (V = 11) |\n",
    "|----------------|\n",
    "| eat            |\n",
    "| broccoli       |\n",
    "| banana         |\n",
    "| spinach        |\n",
    "| lunch          |\n",
    "| chinchilla     |\n",
    "| kitten         |\n",
    "| cute           |\n",
    "| sister         |\n",
    "| today          |\n",
    "| hamster        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "The above is all the unique words in our vocabulary across all documents.  These are the words for which we will assign topics to based on our set of topics (k).<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the β (Beta) Distribution:\n",
    "<br>\n",
    "β — A distribution of words, one for each topic.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-kiyi{font-weight:bold;border-color:inherit;text-align:left}\n",
    ".tg .tg-u0o7{font-weight:bold;text-decoration:underline;border-color:inherit;text-align:left;vertical-align:top}\n",
    ".tg .tg-xldj{border-color:inherit;text-align:left}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    ".tg .tg-fymr{font-weight:bold;border-color:inherit;text-align:left;vertical-align:top}\n",
    ".tg .tg-xwhs{font-weight:bold;text-decoration:underline;border-color:inherit;text-align:left}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-xldj\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-u0o7\">Words</td>\n",
    "    <td class=\"tg-fymr\">eat</td>\n",
    "    <td class=\"tg-fymr\">broccoli</td>\n",
    "    <td class=\"tg-fymr\">banana</td>\n",
    "    <td class=\"tg-fymr\">spinach</td>\n",
    "    <td class=\"tg-fymr\">lunch</td>\n",
    "    <td class=\"tg-fymr\">chinchilla</td>\n",
    "    <td class=\"tg-fymr\">kitten</td>\n",
    "    <td class=\"tg-fymr\">cute</td>\n",
    "    <td class=\"tg-fymr\">sister</td>\n",
    "    <td class=\"tg-fymr\">today</td>\n",
    "    <td class=\"tg-fymr\">hamster</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xwhs\">Topics</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-kiyi\">1</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-fymr\">2</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "To compute the Beta distribution, we look at our initial topic assignment for each word in each document.<br>\n",
    "\n",
    "We count the # of times each word is associated with a particular topic across all documents.<br>\n",
    "\n",
    "For example, we see here that the word \"eat\" appears two times in total.  The first time \"eat\" appears, it is associated with topic 1.  The second time \"eat\" appears, it is associated with topic 2.<br>\n",
    "\n",
    "Therefore, we put a 1 in the cell corresponding to Topic 1 and the Word \"eat\" and we also put a 1 in the cell corresponding to Topic 2 and the Word \"eat\".<br>\n",
    "\n",
    "We do this for each word (w) in our vocabulary (V) across all documents (d) based on our initial topic assignment for each word in each document.<br>\n",
    "\n",
    "Note: \"placeholder\" simply means that we are not inputting an actual value for the sake of simplicity in this example.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the θ (Theta) Distribution:\n",
    "<br>\n",
    "θ — A distribution of topics, one for each document.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-kiyi{font-weight:bold;border-color:inherit;text-align:left}\n",
    ".tg .tg-xldj{border-color:inherit;text-align:left}\n",
    ".tg .tg-xwhs{font-weight:bold;text-decoration:underline;border-color:inherit;text-align:left}\n",
    ".tg .tg-fymr{font-weight:bold;border-color:inherit;text-align:left;vertical-align:top}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-xldj\"></th>\n",
    "    <th class=\"tg-xwhs\">Documents</th>\n",
    "    <th class=\"tg-kiyi\">1</th>\n",
    "    <th class=\"tg-fymr\">2</th>\n",
    "    <th class=\"tg-fymr\">3</th>\n",
    "    <th class=\"tg-fymr\">4</th>\n",
    "    <th class=\"tg-fymr\">5</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xwhs\">Topics</td>\n",
    "    <td class=\"tg-xldj\"></td>\n",
    "    <td class=\"tg-xldj\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-kiyi\">1</td>\n",
    "    <td class=\"tg-xldj\"></td>\n",
    "    <td class=\"tg-xldj\">2</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-fymr\">2</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">2</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "To compute the Theta distribution, we look at our initial topic assignment for each word in each document.<br>\n",
    "\n",
    "We count the # of times each document is associated with each topic in our set of topics.<br>\n",
    "\n",
    "Since we have three words per document, we see that Document 1 is associated with Topic 1 two times since two words are associated with Topic 1.  We also see that Document 1 is associated with Topic 2 one time since one word is associated with Topic 2.<br>\n",
    "\n",
    "Therefore, we put a 2 in the cell corresponding to Topic 1 and Document 1 and we also put a 1 in the cell corresponding to Topic 2 and Document 1.<br>\n",
    "\n",
    "We do this for each topic (z) for each document (d) based on our initial topic assignment for each word in each document.<br>\n",
    "\n",
    "Note: \"placeholder\" simply means that we are not inputting an actual value for the sake of simplicity in this example.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the initial topic assignment for each word in each document:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-s268{text-align:left}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-s268\"></th>\n",
    "    <th class=\"tg-s268\"></th>\n",
    "    <th class=\"tg-s268\"></th>\n",
    "    <th class=\"tg-0lax\"></th>\n",
    "    <th class=\"tg-0lax\"></th>\n",
    "    <th class=\"tg-0lax\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\">Document 1</td>\n",
    "    <td class=\"tg-0lax\">Document 2</td>\n",
    "    <td class=\"tg-0lax\">Document 3</td>\n",
    "    <td class=\"tg-0lax\">Document 4</td>\n",
    "    <td class=\"tg-0lax\">Document 5</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Broccoli-Topic 1</td>\n",
    "    <td class=\"tg-0lax\">1 X 2 = 2</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Broccoli-Topic 2</td>\n",
    "    <td class=\"tg-0lax\">1 X 1 = 1</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "In order to update our initial topic assignments for each word in each document, we look at the Beta and Theta distributions we calculated previously.<br>\n",
    "\n",
    "Notice that \"broccoli\" is associated with Topic 1 one time and Topic 2 one time in the Beta distribution while Document 1 is associated with Topic 1 two times and Topic 2 one time in the Theta distribution.<br>\n",
    "\n",
    "Now, to calculate the new topic (z) assignment for the word (w) \"broccoli\", we do some simple arithmetic operations.<br>\n",
    "\n",
    "We multiply the value in the cell associated with Topic 1 and \"broccoli\" in the Beta distribution with the value in the cell associated with Topic 1 and Document 1 in the Theta distribution.  This gives us 1 X 2 = 2.<br>\n",
    "\n",
    "We then multiple the value in the cell associated with Topic 2 and \"broccoli\" in the Beta distribution with the value in the cell associated with Topic 2 and Document 1 in the Theta distribution.  This gives us 1 X 1 = 1.<br>\n",
    "\n",
    "##### Important Note:  This process is repeated for each word in each document BEFORE moving on to the next document.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-88nc{font-weight:bold;border-color:inherit;text-align:center}\n",
    ".tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-uys7{border-color:inherit;text-align:center}\n",
    ".tg .tg-y2k2{font-weight:bold;text-decoration:underline;border-color:inherit;text-align:center}\n",
    ".tg .tg-7btt{font-weight:bold;border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-9353{font-weight:bold;text-decoration:underline;border-color:inherit;text-align:center;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-uys7\"></th>\n",
    "    <th class=\"tg-y2k2\">Documents (M = 5)</th>\n",
    "    <th class=\"tg-88nc\">Document 1</th>\n",
    "    <th class=\"tg-7btt\">Document 2</th>\n",
    "    <th class=\"tg-7btt\">Document 3</th>\n",
    "    <th class=\"tg-7btt\">Document 4</th>\n",
    "    <th class=\"tg-7btt\">Document 5</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-9353\">Words (in Vocabulary)</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">Eat-Topic 1</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">Eat-Topic 2</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">Broccoli-Topic 1</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\">1 X 2 = 2 --&gt; 2 / (2 + 1) = 2/3</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">Broccoli-Topic 2</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\">1 X 1 = 1 --&gt; 1 / (2 + 1) = 1/3</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">...</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">...</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">Hamster-Topic 1</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">Hamster-Topic 2</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "    <td class=\"tg-c3ow\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Next, we sum the resulting values for the above multiplications to obtain (1 X 2) + (1 X 1) = 3.<br>\n",
    "\n",
    "Then, we divide the resulting values for each of the above multiplications by the value of the sum, 3.<br>\n",
    "\n",
    "Therefore, we have obtained probability values by which we use to select a new topic to assign to the word \"broccoli\".<br>\n",
    "\n",
    "In this case, they are a 2/3 = 0.6666667 chance that we assign \"broccoli\" to Topic 1 in Document 1 and a 1/3 = 0.33333333 chance that we assign \"broccoli\" to Topic 2 in Document 1.<br>\n",
    "\n",
    "Notice that we are assigning a new topic to the word \"broccoli\" in Document 1 according to PROBABILITIES that are calculated using the arithmetic operations above.<br>\n",
    "\n",
    "We are NOT simply arbitrarily assigning a new topic (z) to the word (w) \"broccoli\".  Everything is based on the Beta and Theta distributions and the conditional probabilities in the LDA pseudocode described above.<br>\n",
    "\n",
    "We repeat this for each word (w) in our vocabulary (V) for each document (d) in our set of documents (M).<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated topic assignment for \"broccoli\" in Document 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-c3ow\">Documents (M = 5, N = 3)</th>\n",
    "    <th class=\"tg-c3ow\">Word 1</th>\n",
    "    <th class=\"tg-c3ow\">Word 2</th>\n",
    "    <th class=\"tg-c3ow\">Word 3</th>\n",
    "    <th class=\"tg-c3ow\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Doc 1 Word Topic Assignment--&gt;</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\">2 --&gt; 1</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Document 1</td>\n",
    "    <td class=\"tg-c3ow\">eat</td>\n",
    "    <td class=\"tg-c3ow\">broccoli</td>\n",
    "    <td class=\"tg-c3ow\">banana</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Doc 2 Word Topic Assignment--&gt;</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Document 2</td>\n",
    "    <td class=\"tg-c3ow\">banana</td>\n",
    "    <td class=\"tg-c3ow\">spinach</td>\n",
    "    <td class=\"tg-c3ow\">lunch</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Doc 3 Word Topic Assignment--&gt;</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Document 3</td>\n",
    "    <td class=\"tg-c3ow\">chinchilla</td>\n",
    "    <td class=\"tg-c3ow\">kitten</td>\n",
    "    <td class=\"tg-c3ow\">cute</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Doc 4 Word Topic Assignment--&gt;</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Document 4</td>\n",
    "    <td class=\"tg-c3ow\">sister</td>\n",
    "    <td class=\"tg-c3ow\">kitten</td>\n",
    "    <td class=\"tg-c3ow\">today</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Doc 5 Word Topic Assignment--&gt;</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\">2</td>\n",
    "    <td class=\"tg-c3ow\">1</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Document 5</td>\n",
    "    <td class=\"tg-c3ow\">hamster</td>\n",
    "    <td class=\"tg-c3ow\">eat</td>\n",
    "    <td class=\"tg-c3ow\">broccoli</td>\n",
    "    <td class=\"tg-c3ow\"></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "Look to the table above for the new topic (z) assigned to the word (w) \"broccoli\" ASSUMING that in using the probabilities we just calculated we decide on reassigning \"broccoli\" to Topic 1 in Document 1.<br>\n",
    "\n",
    "It is important to know that we could also have assigned \"broccoli\" to Topic 2 instead.  However, based on the calculated probabilities for each topic (z) in our set of topics (k) it is far more likely that a randomized selection will select Topic 1 rather than Topic 2 (since Topic 1 = 2/3 chance and Topic 2 = 1/3 chance).<br>\n",
    "\n",
    "In an actual implementation of the LDA model, we would do this reassignment for each word (w) in each document (d) based on the probabilities calculated for each word (w) using the Beta and Theta distributions.<br>\n",
    "\n",
    "However, we are not done yet with just the first iteration of the LDA algorithm.  We still need to update the values for the Beta and Theta distributions for the next iteration of the LDA algorithm.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Updated β (Beta) Distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-xldj{border-color:inherit;text-align:left}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-xldj\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">Words</td>\n",
    "    <td class=\"tg-0pky\">eat</td>\n",
    "    <td class=\"tg-0pky\">broccoli</td>\n",
    "    <td class=\"tg-0pky\">banana</td>\n",
    "    <td class=\"tg-0pky\">spinach</td>\n",
    "    <td class=\"tg-0pky\">lunch</td>\n",
    "    <td class=\"tg-0pky\">chinchilla</td>\n",
    "    <td class=\"tg-0pky\">kitten</td>\n",
    "    <td class=\"tg-0pky\">cute</td>\n",
    "    <td class=\"tg-0pky\">sister</td>\n",
    "    <td class=\"tg-0pky\">today</td>\n",
    "    <td class=\"tg-0pky\">hamster</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">Topics</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-xldj\">1</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1 --&gt; 2</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">2</td>\n",
    "    <td class=\"tg-0pky\"></td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1 --&gt; 0</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "    <td class=\"tg-0pky\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Note that the cell associated with Topic 1 and \"broccoli\" has changed from 1 --> 2 and that the cell associated with Topic 2 and \"broccoli\" has change from 1 --> 0.<br>\n",
    "\n",
    "Refer to the updated topic assignment for \"broccoli\" in Document 1 in the table in the previous section.<br>\n",
    "\n",
    "In that table, notice that the word (w) \"broccoli\" is now only associated with Topic 1 across all documents (d) and that the word (w) \"broccoli\" occurs twice across all documents (d).<br>\n",
    "\n",
    "Therefore, we update the cell associated with Topic 1 and \"broccoli\" in the Beta distribution to 2 and we also update the cell associated with Topic 2 and \"broccoli\" in the Beta distribution to 0.<br>\n",
    "\n",
    "We would do this for all words (w) in our vocabulary (V) for all topics (z) in our set of topics (k).<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Updated θ (Theta) Distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-s268{text-align:left}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-s268\"></th>\n",
    "    <th class=\"tg-s268\">Documents</th>\n",
    "    <th class=\"tg-s268\">1</th>\n",
    "    <th class=\"tg-0lax\">2</th>\n",
    "    <th class=\"tg-0lax\">3</th>\n",
    "    <th class=\"tg-0lax\">4</th>\n",
    "    <th class=\"tg-0lax\">5</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s268\">Topics</td>\n",
    "    <td class=\"tg-s268\"></td>\n",
    "    <td class=\"tg-s268\"></td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s268\">1</td>\n",
    "    <td class=\"tg-s268\"></td>\n",
    "    <td class=\"tg-s268\">2 --&gt; 3</td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">2</td>\n",
    "    <td class=\"tg-0lax\"></td>\n",
    "    <td class=\"tg-0lax\">1 --&gt; 0</td>\n",
    "    <td class=\"tg-0lax\">2</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "    <td class=\"tg-0lax\">placeholder</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Note that the cell associated with Topic 1 and Document 1 has changed from 2 --> 3 and that the cell associated with Topic 2 and Document 1 has changed from 1 --> 0.<br>\n",
    "\n",
    "Refer to the updated topic assignment for \"broccoli\" in Document 1 in the table in the previous section.<br>\n",
    "\n",
    "In that table, notice that Document 1 contains 3 words (N) that are now all associated with Topic 1.  So, there are no words in Document 1 that are associated with Topic 2.<br>\n",
    "\n",
    "Therefore, we update the cell associated with Topic 1 and Document 1 in the Theta distribution to 3 and we also update the cell associated with Topic 2 and Document 1 in the Theta distribution to 0.<br>\n",
    "\n",
    "We would do this for all documents (d) for all topics (z) in our set of topics (k).<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are finally finished with the FIRST iteration of the LDA algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "We would now start at Step 2 and rinse + repeat until we have exhausted our allocated compute time.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Latent Dirichlet Allocation on SLO Twitter Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of LDA's:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "</p>Our implementation of LDA's utilizes the Scikit-Learn LatentDirichletAllocation class and the Python \"lda\" library.  We utilize Scikit-Learn's GridSearchCV class to perform an exhaustive grid search for the optimal hyper parameters to fit our Twitter dataset.  We preprocess our raw Twitter dataset before running multiple iterations of the LDA algorithm with the following specified number of topics: 3, 6, 12, and 20.  We limit each topic to the top 10 words that describe that topic.</p>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "Tweet preprocessing is done via a custom library imported as \"lda_util\" using \"slo_lda_topic_extraction_utility_functions.py\"<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Adjust log verbosity levels as necessary.<br>\n",
    "\n",
    "Set to \"DEBUG\" to view all debug output.<br>\n",
    "Set to \"INFO\" to view useful information on dataframe shape, etc.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Resources Used:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation\n",
    "https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Import libraries.\n",
    "import logging as log\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Miscellaneous parameter adjustments for pandas and python.\n",
    "pd.options.display.max_rows = 10\n",
    "# pd.options.display.float_format = '{:.1f}'.format\n",
    "pd.set_option('precision', 7)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "\"\"\"\n",
    "Turn log statements for various sections of code on/off.\n",
    "(adjust log level as necessary)\n",
    "\"\"\"\n",
    "log.basicConfig(level=log.INFO)\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import preprocessing functions and preprocess Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "We preprocess our Twitter dataset as follows:<br>\n",
    "\n",
    "1) Downcase all text.<br>\n",
    "2) Remove \"RT\" tags.\n",
    "3) Remove URL's and replace with slo_url.<br>\n",
    "4) Remove Tweet mentions and replace with slo_mention.<br>\n",
    "5) Remove Tweet hashtags and replace with slo_hashtag.<br>\n",
    "6) Remove all punctuation in the Tweet.<br>\n",
    "7) Remove all words we find to be irrelevant for topic extraction from the Tweet.<br>\n",
    "8) Save the preprocessed Tweets to a external CSV file for use in LDA topic extraction.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom utility functions.\n",
    "import slo_lda_topic_extraction_utility_functions as lda_util\n",
    "\n",
    "# lda_util.tweet_dataset_preprocessor(\"D:/Dropbox/summer-research-2019/datasets/dataset_20100101-20180510_tok_PROCESSED.csv\",\n",
    "#                                     \"D:/Dropbox/summer-research-2019/datasets/dataset_20100101-20180510_tok_LDA_PROCESSED2.csv\", \"tweet_t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "The first parameter in our function call specifies the file path to the datset to be preprocessed.  The second parameter specifies the location to save the CSV file to.  The 3rd parameter specifies the name of the column in the dataset that contains the original Tweet text.<br>\n",
    "\n",
    "Refer to URL link for the codebase to the utility functions used above for data preprocessing and LDA topic extraction:\n",
    "\n",
    "https://github.com/J-Jinn/Summer-Research-2019/blob/master/slo_lda_topic_extraction_utility_functions.py\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and prepare the preprocessed dataset for use in LDA topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "Refer to the code comments for the specific steps performed.<br>\n",
    "Note that we have to use absolute file paths in Jupyter notebook as opposed to relative file paths in PyCharm.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "\n",
      "INFO:root:The shape of our preprocessed SLO dataframe with NaN (empty) rows dropped:\n",
      "INFO:root:(653094, 1)\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:The columns of our preprocessed SLO dataframe with NaN (empty) rows dropped:\n",
      "INFO:root:<bound method NDFrame.head of                                                   tweet_t\n",
      "653632  breaking deputy pm barnaby joyce warns that la...\n",
      "362021  yesterdays biggest risers were ltd up 1456 sla...\n",
      "480819  last year three companies paid zero tax on 514...\n",
      "46623   bulga residents take fight against tintos ridi...\n",
      "439289  annastacia palaszczuk confronted by antiadani ...\n",
      "...                                                   ...\n",
      "4456    aicle galillee coal project a can of financial...\n",
      "85795   those indian influencer accounts are still swa...\n",
      "263707  oh girl you have no idea ahah and its cause sh...\n",
      "265613  infographic heres exactly what adanis mine mea...\n",
      "285820  cuts 700 jobs in queensland coal about 700 job...\n",
      "\n",
      "[653094 rows x 1 columns]>\n",
      "INFO:root:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset.\n",
    "# tweet_dataset_processed = \\\n",
    "#     pd.read_csv(\"datasets/dataset_20100101-20180510_tok_LDA_PROCESSED.csv\", sep=\",\")\n",
    "\n",
    "# Import the dataset.\n",
    "tweet_dataset_processed = \\\n",
    "    pd.read_csv(\"D:/Dropbox/summer-research-2019/datasets/dataset_20100101-20180510_tok_LDA_PROCESSED.csv\", sep=\",\")\n",
    "\n",
    "# Reindex and shuffle the data randomly.\n",
    "tweet_dataset_processed = tweet_dataset_processed.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_dataframe_processed = pd.DataFrame(tweet_dataset_processed)\n",
    "\n",
    "# Drop any NaN or empty Tweet rows in dataframe (or else CountVectorizer will blow up).\n",
    "tweet_dataframe_processed = tweet_dataframe_processed.dropna()\n",
    "\n",
    "# Print shape and column names.\n",
    "log.info(\"\\n\")\n",
    "log.info(\"The shape of our preprocessed SLO dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(tweet_dataframe_processed.shape)\n",
    "log.info(\"\\n\")\n",
    "log.info(\"The columns of our preprocessed SLO dataframe with NaN (empty) rows dropped:\")\n",
    "log.info(tweet_dataframe_processed.head)\n",
    "log.info(\"\\n\")\n",
    "\n",
    "# Reindex everything.\n",
    "tweet_dataframe_processed.index = pd.RangeIndex(len(tweet_dataframe_processed.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_dataframe_processed_column_names = ['Tweet']\n",
    "\n",
    "# Rename column in dataframe.\n",
    "tweet_dataframe_processed.columns = tweet_dataframe_processed_column_names\n",
    "\n",
    "# Create input feature.\n",
    "selected_features = tweet_dataframe_processed[tweet_dataframe_processed_column_names]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "# Check what we are using as inputs.\n",
    "log.debug(\"\\n\")\n",
    "log.debug(\"The Tweets in our input feature:\")\n",
    "log.debug(processed_features['Tweet'])\n",
    "log.debug(\"\\n\")\n",
    "\n",
    "# Create feature set.\n",
    "slo_feature_set = processed_features['Tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "The above log.INFO messages depict the shape and contents of the preprocessed dataframe after dropping any rows that are just \"NaN\", indicating the Tweet was full of irrelevant words and is now empty due to removal of those irrelevant words.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the topic extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "We use the Scikit-Learn CountVectorizer class to vectorize our categorical Tweet data.  We set the max_features parameter to 1000 to indicate a maximum vocabulary of 1k words based on the 1000 words with the highest term frequencies.  We set the stop_words parameter to \"English\" to indicate we would like to remove English stop words based on a built-in library of stop words.  We set the min_df and max_df parameters to indicate the words with the threshold term frequencies at which we ignore those words and do not include them in our vocabulary.<br>\n",
    "\n",
    "We use the Scikit-Learn LatentDirichletAllocation class with the below hyper parameters to train on and fit to our Tweet data.  The parameter n_topics controls the # of topics we would like to extract for topic modeling.  The parameter max_iter controls the # of iterations to perform LDA before we cease.  The parameter learning_method controls the method by which we update the words in our topics.  <br>\n",
    "\n",
    "We use a utility function to display Topics 1-20 and the top 10 words associated with each Topic.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "tax pay energy thanks ceo latest high corporate story office\n",
      "Topic 1:\n",
      "labor australian foescue going federal election said giant political hey\n",
      "Topic 2:\n",
      "stop coal rail fund line clean seam gas protest global\n",
      "Topic 3:\n",
      "news make right lnp wants did thats local shares watch\n",
      "Topic 4:\n",
      "climate public money change fight business world national policy doing\n",
      "Topic 5:\n",
      "jobs new coal iron ore plans beach create really end\n",
      "Topic 6:\n",
      "time year mines paid industry price look australias companies profit\n",
      "Topic 7:\n",
      "turnbull funding oil years banks biggest planet cuts despite dead\n",
      "Topic 8:\n",
      "queensland india coal power cou minister alp jobs approval environment\n",
      "Topic 9:\n",
      "ahead adani council come carbon politicians fossil week production mega\n",
      "Topic 10:\n",
      "project gas slocashn narrabri vote getting gov noh pipeline sign\n",
      "Topic 11:\n",
      "water say basin galilee way barnaby joyce repo canavan let\n",
      "Topic 12:\n",
      "want know coal state group campaign oppose join forest tell\n",
      "Topic 13:\n",
      "government environmental labor cut loan job disaster veto coalition goes\n",
      "Topic 14:\n",
      "coal future adanis point deal build po taxpayer pm massive\n",
      "Topic 15:\n",
      "loan govt need wont greens indian naif use premier board\n",
      "Topic 16:\n",
      "company day billion help breaking huge native title bank times\n",
      "Topic 17:\n",
      "suppo dont coal good shoen does doesnt think community money\n",
      "Topic 18:\n",
      "reef people barrier coal save country green demand funds message\n",
      "Topic 19:\n",
      "adanis farmers action risk coalmine work land water free case\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model.\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(slo_feature_set)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# Run LDA.\n",
    "lda = LatentDirichletAllocation(n_topics=20, max_iter=5, learning_method='online', learning_offset=50.,\n",
    "                                random_state=0).fit(tf)\n",
    "\n",
    "# Display the top words for each topic.\n",
    "lda_util.display_topics(lda, tf_feature_names, 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "We cannot seem to find any strong correlation between the 10 words in each Topic such that we could assign an English descriptor to each topic, such as \"economic\", \"environmental\", \"social\", etc.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from a different execution of LDA topic extraction on our dataset (using PyCharm):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "These results were obtained using the exact same code-base and hyper parameters, only it was done within PyCharm rather than the Jupyter Notebook.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Topic 0:\n",
    "loan rail lnp federal line fund think naif use story\n",
    "Topic 1:\n",
    "need group community carbon country stand high week end wrong\n",
    "Topic 2:\n",
    "tax breaking paid industry ceo green china financial hey licence\n",
    "Topic 3:\n",
    "government new money day environmental barnaby joyce taxpayers plans giving\n",
    "Topic 4:\n",
    "news funding foescue shoen help thanks pm beach bank cut\n",
    "Topic 5:\n",
    "suppo going world wont greens work vote let companies dollars\n",
    "Topic 6:\n",
    "time farmers year look global fossil profit video police best\n",
    "Topic 7:\n",
    "people say campaign free indian giant local australias corruption prices\n",
    "Topic 8:\n",
    "gas coal public slocashn project deal does adani risk biggest\n",
    "Topic 9:\n",
    "action fight state did join decision protest doesnt getting read\n",
    "Topic 10:\n",
    "climate project coal change repo environment price shares doing lost\n",
    "Topic 11:\n",
    "minister election build years native title canavan away planet owners\n",
    "Topic 12:\n",
    "reef turnbull cou barrier coal ahead know adanis wants approval\n",
    "Topic 13:\n",
    "india company pay coal alp business right oil oppose win\n",
    "Topic 14:\n",
    "coal good power make mines iron ore new po protect\n",
    "Topic 15:\n",
    "jobs australian needs create thats billion tell got investment workers\n",
    "Topic 16:\n",
    "adanis energy future coal land taxpayer said clean destroy gov\n",
    "Topic 17:\n",
    "want govt dont point coal massive politicians infrastructure protesters subsidies\n",
    "Topic 18:\n",
    "stop water queensland narrabri coal disaster corporate unlimited live tonight\n",
    "Topic 19:\n",
    "labor coalmine coal galilee way huge basin job pollution townsville\n",
    "INFO:root:The time taken to perform the operation is: \n",
    "INFO:root:452.49793434143066\n",
    "INFO:root:\n",
    "\n",
    "\n",
    "Process finished with exit code 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Though the results are different, the top 10 words for each of the 20 different Topics still lack any strong association to each other.  We still cannot easily assign any English descriptors to each topic.<br>\n",
    "\n",
    "We decided to time the LDA model.  It takes around 450 seconds or so to finish LDA topic extraction per execution, so it is not a particularly fast process.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from LDA topic extraction for 3 topics (using PyCharm):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Topic 0:\n",
    "coal tax loan australian govt money pay turnbull adanis government\n",
    "Topic 1:\n",
    "coal stop labor reef want climate time people dont need\n",
    "Topic 2:\n",
    "coal jobs gas water project adanis rail news cou farmers\n",
    "INFO:root:The time taken to perform the operation is: \n",
    "INFO:root:713.6182627677917\n",
    "INFO:root:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "  \n",
    "Again, we can't really discern any noticeable patterns among the top 10 words for each topic.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from LDA topic extraction for 6 topics (using PyCharm):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Topic 0:\n",
    "coal queensland money dont adanis rail funding loan fund government\n",
    "Topic 1:\n",
    "gas people coal tax project minister paid narrabri iron news\n",
    "Topic 2:\n",
    "labor coal stop india loan time pay federal year greens\n",
    "Topic 3:\n",
    "coal new reef water company point slocashn barrier adanis make\n",
    "Topic 4:\n",
    "climate good lnp world basin change ahead say turnbull alp\n",
    "Topic 5:\n",
    "jobs coal suppo govt need power foescue going cou action\n",
    "INFO:root:The time taken to perform the operation is: \n",
    "INFO:root:506.35622787475586\n",
    "INFO:root:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "As stated above.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from LDA topic extraction for 12 topics (using PyCharm):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Topic 0:\n",
    "coal power india environmental galilee fight repo po plans join\n",
    "Topic 1:\n",
    "fund know billion wants business years pm giant really cut\n",
    "Topic 2:\n",
    "water breaking protect local cou basin doesnt slomentions high licence\n",
    "Topic 3:\n",
    "gas coal new news people oil huge seam canavan renewables\n",
    "Topic 4:\n",
    "labor going foescue shoen help election iron ore said group\n",
    "Topic 5:\n",
    "jobs loan australian dont want government point money ahead greens\n",
    "Topic 6:\n",
    "stop coal say make world action risk work biggest people\n",
    "Topic 7:\n",
    "need year day deal ceo banks come look politicians getting\n",
    "Topic 8:\n",
    "reef coal farmers land barrier mines thanks new use indian\n",
    "Topic 9:\n",
    "adanis queensland turnbull govt good company lnp right labor coalmine\n",
    "Topic 10:\n",
    "project coal suppo climate rail public adanis change federal funding\n",
    "Topic 11:\n",
    "tax pay time energy future paid clean beach campaign oppose\n",
    "INFO:root:The time taken to perform the operation is: \n",
    "INFO:root:468.9760229587555\n",
    "INFO:root:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Ditto.<br>\n",
    "\n",
    "Of interesting note is that it appears to take longer to perform LDA topic extraction specifying fewer topics over more topics.  We surmise this is because we have a large dataset of 650k+ Tweets which translates to 650k+ different documents in our corpus.  Therefore, it would take the algorithm less time if it could simply assign 650k+ documents to 650k+ different topics rather than having to assign 650k+ documents to a mere 3 topics or in general a much smaller number of topics in comparison to the number of documents.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exhaustive grid search for Scikit-Learn LDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "We use Scikit-Learn's Pipeline Class to construct a pipeline consisting of the CounteVectorizer and LatentDirichletAllocation classes.<br>\n",
    "\n",
    "The \"parameters\" dictionary determine all the possible combinations of hyper parameters we will test in order to find the optimal hyper parameters for the Scikit-Learn LDA model.<br>\n",
    "\n",
    "The grid search is performed by fitting on the Twitter data we wish to use for topic extraction.<br>\n",
    "\n",
    "The optimal hyper parameters are displayed via \"log.info\" messages so the log verbosity level must be set appropriately to view them.<br>\n",
    "\n",
    "We recommend executing this only on a supercomputer as otherwise it will take a extremely long time to finish, depending on the number of possible combinations of hyper parameters as defined below.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What parameters do we search for?\n",
    "lda_search_parameters = {\n",
    "    # 'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "    'clf__n_components': [1, 5, 10, 15],\n",
    "    'clf__doc_topic_prior': [None],\n",
    "    'clf__topic_word_prior': [None],\n",
    "    'clf__learning_method': ['batch', 'online'],\n",
    "    'clf__learning_decay': [0.5, 0.7, 0.9],\n",
    "    'clf__learning_offset': [5, 10, 15],\n",
    "    'clf__max_iter': [1],\n",
    "    'clf__batch_size': [64, 128, 256],\n",
    "    'clf__evaluate_every': [0],\n",
    "    'clf__total_samples': [1e4, 1e6, 1e8],\n",
    "    'clf__perp_tol': [1e-1, 1e-2, 1e-3],\n",
    "    'clf__mean_change_tol': [1e-1, 1e-3, 1e-5],\n",
    "    'clf__max_doc_update_iter': [50, 100, 150],\n",
    "    'clf__n_jobs': [-1],\n",
    "    'clf__verbose': [0],\n",
    "    'clf__random_state': [None],\n",
    "}\n",
    "# lda_util.latent_dirichlet_allocation_grid_search(slo_feature_set, lda_search_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "If running this code snippet on a non workstation PC, you may want to change \"n_jobs=-1\" to \"n_jobs=0\" to prevent Python from utilizing all CPU cores and bogging down your system to unusability for the duration of the search.<br>\n",
    "\n",
    "Refer to URL link for the codebase to the utility functions used above for data preprocessing and LDA topic extraction:\n",
    "\n",
    "https://github.com/J-Jinn/Summer-Research-2019/blob/master/slo_lda_topic_extraction_utility_functions.py\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive grid search for Scikit-Learn LDA using subset of Twitter dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Here, we implement a exhaustive grid search using a smaller subset of the entire Twitter dataset.  This is done as to cut down on the computational time required to finish the search.  We have a large dataset of over 650k+ Tweets so utilizing the full dataset drastically increases the search time.<br>\n",
    "\n",
    "The first parameter for the \"dataframe_subset\" function dictates the full dataset you wish to subset while the second parameter defines the number of rows (examples) desired for the subset of the full dataset.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = lda_util.dataframe_subset(tweet_dataset_processed, 10000)\n",
    "lda_util.latent_dirichlet_allocation_grid_search(data_subset, lda_search_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Placeholder.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Topic Extraction using the \"lda\" library and collapsed Gibbs Sampling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "The code below uses the \"lda\" Python library package that performs LDA topic extraction using collapsed Gibbs Sampling.<br>\n",
    "This is different from the Scikit-Learn implementation that uses online variational inference.<br>\n",
    "Otherwise, the dataset is the same and we are still using Scikit-Learn's CountVectorizer class to vectorize our data.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 653094\n",
      "INFO:lda:vocab_size: 1000\n",
      "INFO:lda:n_words: 3267212\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -33566609\n",
      "INFO:lda:<10> log likelihood: -27759459\n",
      "INFO:lda:<20> log likelihood: -24357244\n",
      "INFO:lda:<30> log likelihood: -23217301\n",
      "INFO:lda:<40> log likelihood: -22887465\n",
      "INFO:lda:<50> log likelihood: -22760021\n",
      "INFO:lda:<60> log likelihood: -22691980\n",
      "INFO:lda:<70> log likelihood: -22648547\n",
      "INFO:lda:<80> log likelihood: -22621636\n",
      "INFO:lda:<90> log likelihood: -22601056\n",
      "INFO:lda:<99> log likelihood: -22580145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: labor stop greens vote alp lnp election shoen suppo win\n",
      "Topic 1: time good going know im really think did dont right\n",
      "Topic 2: beach day people join watch action tour stop bob sydney\n",
      "Topic 3: coal adanis cou point approval federal new green giant light\n",
      "Topic 4: jobs create thousands tourism coal 10000 pm adanis job cou\n",
      "Topic 5: reef coal barrier stop save turnbull coral indian canavan minister\n",
      "Topic 6: tax paid energy australian pay ceo companies donations origin chevron\n",
      "Topic 7: gas project narrabri seam coal forest dam barnaby water joyce\n",
      "Topic 8: climate change future coal energy clean fossil time planet global\n",
      "Topic 9: iron ore foescue oil shares production price prices profit year\n",
      "Topic 10: coal money fund banks funding billion adanis taxpayers project govt\n",
      "Topic 11: tax pay company corporate workers cut use profits debt cuts\n",
      "Topic 12: coal power india new solar mines environmental record company renewables\n",
      "Topic 13: coal australian fuher foescue creek assets stranded asset project maules\n",
      "Topic 14: water farmers basin free licence unlimited aesian groundwater coal suppo\n",
      "Topic 15: want coal dont suppo doesnt need project does shoen adanis\n",
      "Topic 16: land stop native title owners people traditional townsville indigenous rights\n",
      "Topic 17: rail line coal loan galilee adanis basin public funding money\n",
      "Topic 18: loan government turnbull govt naif labor slocashn deal taxpayer queensland\n",
      "Topic 19: coal thanks action latest times council point protesters po work\n"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model.\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(slo_feature_set)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# Train and fit the LDA model.\n",
    "model = lda.LDA(n_topics=20, n_iter=100, random_state=1)\n",
    "model.fit(tf)  # model.fit_transform(X) is also available\n",
    "topic_word = model.topic_word_  # model.components_ also works\n",
    "n_top_words = 10\n",
    "\n",
    "# Display the topics and the top words associated with.\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(tf_feature_names)[np.argsort(topic_dist)][:-(n_top_words + 1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "The results seem to be as incoherent as the Scikit-Learn implementation of LDA topic extraction using online variational inference.<br>\n",
    "\n",
    "It's difficult to see any correlation between the 10 top words for each topic.<br>\n",
    "\n",
    "Here, we are using n_iter=100 (iterations) as the fitting to our Twitter data is a lot faster than the Scikit-Learn implementation where max_iter=5 already takes 450 seconds.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A second set of LDA topic extraction results using the \"lda\" library (in Pycharm) with 1000 iterations:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INFO:lda:n_documents: 653094\n",
    "INFO:lda:vocab_size: 1000\n",
    "INFO:lda:n_words: 3267212\n",
    "INFO:lda:n_topics: 20\n",
    "INFO:lda:n_iter: 1000\n",
    "WARNING:lda:all zero row in document-term matrix found\n",
    "INFO:lda:<0> log likelihood: -33566800\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "INFO:lda:<999> log likelihood: -22482922\n",
    "Topic 0: project coal australian banks bank fund funding adanis finance chinese\n",
    "Topic 1: tax paid pay company corporate energy companies islands cayman australian\n",
    "Topic 2: beach day watch tour video iluka people got work power\n",
    "Topic 3: thanks council townsville protest action latest times people outside protesters\n",
    "Topic 4: rail loan line adanis coal galilee naif basin veto queensland\n",
    "Topic 5: labor stop greens vote alp lnp election shoen want win\n",
    "Topic 6: people land owners traditional farmers coal indigenous creek site local\n",
    "Topic 7: loan canavan minister board matt news abc coalition resources john\n",
    "Topic 8: coal suppo labor want oppose shoen dont poll doesnt voters\n",
    "Topic 9: reef barrier coal stop people destroy save future need protect\n",
    "Topic 10: foescue iron ore oil shares price production prices profit metals\n",
    "Topic 11: dont going good think want really make know need look\n",
    "Topic 12: coal turnbull native title adanis indian government stop point india\n",
    "Topic 13: environmental coal cou disaster dam approval australian federal financial adanis\n",
    "Topic 14: jobs create thousands 10000 coal tourism job cou queensland pm\n",
    "Topic 15: coal money billion govt want fund public turnbull taxpayers taxpayer\n",
    "Topic 16: coal new power india point solar mines adanis po time\n",
    "Topic 17: water free basin farmers unlimited aesian licence groundwater queensland coal\n",
    "Topic 18: climate coal change future energy clean fossil time carbon global\n",
    "Topic 19: gas project coal narrabri barnaby joyce seam water field forest\n",
    "INFO:root:The time taken to perform the operation is: \n",
    "INFO:root:522.370343208313\n",
    "INFO:root:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "These results are from another execution using the same library as the previous results.<br>\n",
    "\n",
    "Again, there's no discernible patterns in the choice of top words across all Topics.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A third set of LDA topic extraction results using the \"lda\" library (in Pycharm) with 1000 iterations and 3 topics:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INFO:lda:n_documents: 653094\n",
    "INFO:lda:vocab_size: 1000\n",
    "INFO:lda:n_words: 3267212\n",
    "INFO:lda:n_topics: 3\n",
    "INFO:lda:n_iter: 1000\n",
    "WARNING:lda:all zero row in document-term matrix found\n",
    "INFO:lda:<0> log likelihood: -26923029\n",
    ".\n",
    ".\n",
    ".\n",
    "INFO:lda:<999> log likelihood: -21700037\n",
    "Topic 0: coal water gas climate reef people stop project adanis farmers\n",
    "Topic 1: coal tax new foescue australian company energy india iron power\n",
    "Topic 2: coal jobs labor loan adanis want money government turnbull suppo\n",
    "INFO:root:The time taken to perform the operation is: \n",
    "INFO:root:212.95609331130981\n",
    "INFO:root:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Same situation.  Difficult to discern any patterns among the top words chosen for each topic.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A fourth set of LDA topic extraction results using the \"lda\" library (in Pycharm) with 1000 iterations and 6 topics:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INFO:lda:n_documents: 653094\n",
    "INFO:lda:vocab_size: 1000\n",
    "INFO:lda:n_words: 3267212\n",
    "INFO:lda:n_topics: 6\n",
    "INFO:lda:n_iter: 1000\n",
    "WARNING:lda:all zero row in document-term matrix found\n",
    "INFO:lda:<0> log likelihood: -30452684\n",
    ".\n",
    ".\n",
    ".\n",
    "INFO:lda:<999> log likelihood: -21989143\n",
    "Topic 0: coal jobs reef barrier energy adanis new future climate need\n",
    "Topic 1: tax foescue coal paid iron ore australian company oil price\n",
    "Topic 2: labor coal want stop suppo shoen greens lnp dont alp\n",
    "Topic 3: coal loan adanis money rail government fund funding public taxpayer\n",
    "Topic 4: coal gas water project farmers narrabri cou land seam native\n",
    "Topic 5: climate coal action people beach day thanks change time join\n",
    "INFO:root:The time taken to perform the operation is: \n",
    "INFO:root:291.4145665168762\n",
    "INFO:root:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Ditto.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A fifth set of LDA topic extraction results using the \"lda\" library (in Pycharm) with 1000 iterations and 12 topics:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INFO:lda:n_documents: 653094\n",
    "INFO:lda:vocab_size: 1000\n",
    "INFO:lda:n_words: 3267212\n",
    "INFO:lda:n_topics: 12\n",
    "INFO:lda:n_iter: 1000\n",
    "WARNING:lda:all zero row in document-term matrix found\n",
    "INFO:lda:<0> log likelihood: -32585851\n",
    ".\n",
    ".\n",
    ".\n",
    "INFO:lda:<999> log likelihood: -22256834\n",
    "Topic 0: reef coal climate barrier future change energy world stop clean\n",
    "Topic 1: foescue iron ore oil shares price ceo news production new\n",
    "Topic 2: labor greens stop want lnp alp suppo vote shoen dont\n",
    "Topic 3: water basin barnaby coal joyce galilee rail free farmers thanks\n",
    "Topic 4: coal money want fund adanis govt banks billion turnbull funding\n",
    "Topic 5: gas coal project water narrabri seam farmers forest pipeline field\n",
    "Topic 6: coal cou stop adanis native title land federal coalmine approval\n",
    "Topic 7: loan coal naif rail queensland minister government line public funding\n",
    "Topic 8: coal new india power point adanis mines solar po project\n",
    "Topic 9: action beach people day join time work watch protest morning\n",
    "Topic 10: tax paid pay company australian companies corporate govt adanis energy\n",
    "Topic 11: jobs create coal dam thousands 10000 disaster tourism reef job\n",
    "INFO:root:The time taken to perform the operation is: \n",
    "INFO:root:404.1787123680115\n",
    "INFO:root:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "And Ditto.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does it work poorly on Tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "##### Based on Derek Fisher's senior project presentation:\n",
    "\n",
    "1) LDA typically works best when the documents are lengthy (large word count) and written in a formal proper style.\n",
    "\n",
    "2) Tweet text is generally very short in length with a max of around 280 characters.\n",
    "\n",
    "3) Tweet text is generally written very informally style-wise.\n",
    "\n",
    "    i) emojis.\n",
    "    ii) spelling errors.\n",
    "    iii) other grammatical errors.\n",
    "    iv) etc.\n",
    "\n",
    "4) The above makes it difficult for the LDA algorithm to discover any prominent underlying hidden structures.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some current-ish research on Twitter topic modeling and topic modeling in general:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "https://www.aclweb.org/anthology/W17-0210\n",
    "\n",
    "\"Twitter Topic Modeling by Tweet Aggregation\"\n",
    "\n",
    "https://www.media.mit.edu/publications/topic-modeling-in-twitter-aggregating-tweets-by-conversations/\n",
    "\n",
    "\"Topic Modeling in Twitter: Aggregating Tweets by Conversations\"\n",
    "\n",
    "https://arxiv.org/ftp/arxiv/papers/1206/1206.3297.pdf\n",
    "\n",
    "\"Hybrid Variational/Gibbs Collapsed Inference in Topic Models\"\n",
    "\n",
    "https://www.researchgate.net/publication/318726050_A_Hierarchical_Topic_Modelling_Approach_for_Tweet_Clustering\n",
    "\n",
    "\"A Hierarchical Topic Modelling Approach forTweet Clustering\"\n",
    "\n",
    "https://www.researchgate.net/publication/262244963_A_biterm_topic_model_for_short_texts\n",
    "\n",
    "\"A Biterm Topic Model for Short Texts\"\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "https://en.wikipedia.org/wiki/Dirichlet_distribution\n",
    "\n",
    "Wikipedia page on Dirichlet distributions.<br>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Plate_notation\n",
    "\n",
    "Wikipedia page on Plate notation.<br>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "\n",
    "Wikipedia page on LDA's.<br>\n",
    "\n",
    "https://www.tablesgenerator.com/markdown_tables\n",
    "\n",
    "Easy-to-use table generator for markdown, html, latex, etc.<br>\n",
    "\n",
    "https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158\n",
    "\n",
    "Utilized two diagrams, formula, and explanation of associated notation on LDA's.<br>\n",
    "\n",
    "https://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/\n",
    "\n",
    "Utilized blog's example as the basis for the explanation of the LDA algorithm pseudocode.<br>\n",
    "\n",
    "https://www.coursera.org/learn/ml-clustering-and-retrieval\n",
    "\n",
    "Information on collapsed Gibbs sampling and variational inference in relation to LDA's.<br>\n",
    "\n",
    "https://www.investopedia.com/terms/p/posterior-probability.asp\n",
    "\n",
    "Explanation of statistical terminology including posterior and prior probability.<br>\n",
    "\n",
    "https://cs.calvin.edu/courses/cs/x95/videos/2018-2019/\n",
    "\n",
    "Used Derek Fisher's explanation of why LDA does not work well on Tweets (with Scikit-Learn standard implementation).<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "## Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - consider implementing NMF - Non-Negative Matrix Factorization for Topic Modeling.\n",
    "\n",
    "https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730\n",
    "\n",
    "https://stackabuse.com/python-for-nlp-topic-modeling/\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
