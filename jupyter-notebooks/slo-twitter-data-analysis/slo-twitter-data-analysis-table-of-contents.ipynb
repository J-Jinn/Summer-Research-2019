{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SLO Twitter Data Analysis - Table of Contents\n",
    "\n",
    "## Joseph Jinn and Keith VanderLinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "</p>This collection of Jupyter Notebook files provide an analysis of Twitter data obtained by CSIRO Data61 from a period of time covering 2010 through 2018.  The Twitter API was utilized to extract the raw Tweet data.  The sections below provide a short summary and hyperlinks to individual Jupyter Notebook files that provide further details on our analysis of different attributes and combinations of attributes in our Twitter Dataset.<br>\n",
    "\n",
    "**IMPORTANT NOTE: We cannot implement any further elments of Shuntaro Yada's Twitter data analysis until we get the CMU Tweet Tagger working on a Linux-based workstation.**\n",
    "\n",
    "**TODO List:**\n",
    "\n",
    "- move all titles before the import libraries and parameter adjustments section.<br>\n",
    "- run all text through Grammerly for spell-checking, etc.<br>\n",
    "- add subtitles for each sub-section in each Jupyter notebook file that provides another graph or text statistics output.\n",
    "\n",
    "**TODO: adjust tick scaling of y-axis labels so that small percentages/values will show up as visible bars along the x-axis.**<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter API Introduction:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    These intros should be conceptual, high-level and short, so you can get rid of the opening summary statements.\n",
    "    1 paragraph - raw JSON file; hierarchically-structured data; twitter/user/geo/?? sub-entities. Here's a lisyt of the most important fields: \n",
    "    - tweet\n",
    "        - id, ...\n",
    "    - user\n",
    "        - id, ...\n",
    "    \n",
    "    \n",
    "**TODO: remove after Professor VanderLinden gives the thumbs up for revised summary.**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "This section introduces the raw JSON hierarchical file structure of the Twitter data.  All Tweets are encapsulated within the main Tweet object which contains the \"entities\", \"user\", and \"retweeted_status\" sub-objects that themselves contain various nested attributes.  The attributes we currently consider of the uptmost important to our research is as follows:\n",
    "\n",
    "- retweeted_derived<br>\n",
    "    - indicates whether the Tweet is a retweet.<br>\n",
    "- company_derived<br>\n",
    "    - associates the Tweet with a company.<br>\n",
    "- text_derived<br>\n",
    "    - full Tweet text.<br>\n",
    "- tweet_url_link_derived<br>\n",
    "    - hyperlink to the actual Tweet on Twitter.<br>\n",
    "- company_derived_designation<br>\n",
    "    - single company Tweet is associated with or \"multiple\" for multi-company.<br>\n",
    "- tweet_text_length_derived<br>\n",
    "    - character count of the Tweet text length.<br>\n",
    "- spaCy_language_detect<br>\n",
    "    - language of the Tweet as determined by \"spacy-langdetect\" python library.<br>\n",
    "- tweet_created_at<br>\n",
    "    - time-date stamp of the Tweet.<br>\n",
    "- tweet_id<br>\n",
    "    - unique ID # of the Tweet.<br>\n",
    "- tweet_retweet_count<br>\n",
    "    - the # of times the Tweet has been retweeted.<br>\n",
    "- user_id<br>\n",
    "    - unique ID of the user (Tweet author).<br>\n",
    "- user_description<br>\n",
    "    - short user written description of himself/herself.<br>\n",
    "- tweet_entities_hashtags<br>\n",
    "    - hashtags present in the Tweet text.<br>\n",
    "- tweet_entities_user_mentions_id<br>\n",
    "    - unique ID of the users mentioned in a Tweet.<br>\n",
    "- retweeted_status_full_text<br>\n",
    "    - full text of the original retweeted Tweet.<br>\n",
    "    \n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Twitter API Introduction](slo-twitter-data-analysis-tweet-api-intro.ipynb#bookmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codebase Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "    Tell me what the basic libraries do, e.g., load JSON/process/output CSV; analysis/graphing functions.\n",
    "    Explain the flow for each of the following TOC sections: CSV creation is done once, then, for each TOC section, we read the CSV and use analysis/graphing functions as needed.\n",
    "    Git rid of version numbering in filenames.\n",
    "    \n",
    "\n",
    "**TODO: remove after Professor VanderLinden gives the thumbs up for revised summary.**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Generally, each section of our data analysis imports the necessary libraries and configures the settings for those libraries before proceeding to import the Twitter dataset in processed CSV format.  We then call the relevant data analysis functions which in turn calls upon any necessary dependencies in the form of utility functions.<br>\n",
    "\n",
    " \"dataset_processor_adapted.py\" reads in the raw JSON file structure by chunks and parses through it to derive and extract various attributes of the Tweets that we are interested in.  We export the results to a CSV dataset file.<br>\n",
    " \n",
    "\"slo_twitter_data_analysis.py\" contains all the analysis code we use in this collection of Jupyter Notebook files.  It has dependencies on the \"slo_twitter_data_analysis_utility_functions.py\" file that contains a variety of graphing helper functions among other functions.<br>  \n",
    "\n",
    "\"slo_twitter_data_analysis_utility_functions\" also contains various functions that can flatten nested attributes, extract individual or specific attributes, and derive new attributes, using the CSV dataset file post dataset creation.  This allows us to readily manipulate our dataset in a computationally inexpensive manner (requires less time).<br>\n",
    "\n",
    "**TODO: remove file version numbers once Jupyter Notebook files are nearing completion)**\n",
    "\n",
    "**TODO: reorder sections in Jupyter Notebook file if necessary.**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Codebase Introduction](slo-twitter-data-analysis-codebase-intro.ipynb#bookmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Language Statistics and Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "    move this earlier, before the company assignment. \n",
    "    we should be dropping non-English tweets (I think). Give some stats on this and decide what to do.\n",
    "   \n",
    "\n",
    "**TODO: remove after Professor VanderLinden gives the thumbs up for revised summary.**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "We have decided to drop non-English Tweets from our Twitter dataset.  They do not comprise much of our dataset and will only increase the difficulty of topic extraction.  Approximately 96.6% of our Tweets are English and 3.4% of our Tweets are non-English.  We used the results of our \"spacy-langdetect\" library for language detection.<br>\n",
    "\n",
    "Addendum:<br>\n",
    "\n",
    "The \"spaCy-langdetect\" library does a decent job of identifying the language of the Tweet text in comparison to the Twitter API.  However, it is inferior to using \"textblob\", another Python library, which uses the Google Translate API to perform language detection.  Unfortunately, Google Translate is no longer free and is now a paid service.  Thus, we are forced to find free alternatives.  In the future, we may also use the \"polyglot\" library for Tweet text language detection if we can get it working on our Windows workstation or a Linux workstation.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Language Statistics](slo-twitter-data-analysis-language-statistics.ipynb#bookmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company Assignments for Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "    95%? of tweets have a single company assignment (see the sub-notebook for the assignment rules); others are assiated with multiple companies.\n",
    "    10%? of the multiple company tweets are not that interesting (stock info tweets for the energy sector), but others are nore useful (individual tweets about the industry in general). Thus we combined multi-company tweets into a single \"multiple\" company category.\n",
    "    The analysis sections below will show analysis for aggregate, company-specific and multiple company tweets.\n",
    "    \n",
    "\n",
    "**TODO: remove after Professor VanderLinden gives the thumbs up for revised summary.**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "98.76526268372646% of tweets have a single company assignment (see the sub-notebook for the assignment rules).  1.2347373162735336% of the others are assiated with multiple companies.<br>\n",
    "\n",
    "10%? of the multiple company tweets are not that interesting (stock info tweets for the energy sector), but others are nore useful (individual tweets about the industry in general). Thus we combined multi-company tweets into a single \"multiple\" company category.<br>\n",
    "    \n",
    "The analysis sections below will show analysis for aggregate, company-specific and multiple company tweets.  As a result of this analysis, we have decided to include Tweets with multiple company associations as part of our dataset.  A small portion are not particularly useful but the majority of them do provide some sort of stance/sentiment on SLO mining companies.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Single v. Multi Company Associated Tweets](slo-twitter-data-analysis-one-versus-multiple-companies-statistics.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Statistics and Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "    Indicate that we've decided to lump the years together for now. We could consider bucketing the data by time blocks to see a flow of topics over time.\n",
    "    \n",
    "??% Tweets in our dataset are associated with \"Adani\" and are in the 2017-2018 time period.  The Tweets for the other companies in our dataset are far less numerous.  Some companies show a even distribution across the years (companies?) while others are skewed more towards the left (indicating they are older Tweets, ??) or skewed more towards the right (indicating they are newer Tweets, ??).<br>\n",
    "\n",
    "**TODO: remove after Professor VanderLinden gives the thumbs up for revised summary.**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Approximately 64% of our Tweets are associated with \"Adani\" and most of those are in the 2017-2018 time period.  The Tweets for the other companies in our dataset are far less numerous.  Fortescue, iluka, newmont, and woodside show a relatively even distribution across the years.  Multi-company Tweets, bhp, oilsearch, and riotinto show a distribution somewhat skewed to the right (indicating newer Tweets).  Cuesta and whitehaven show a distribution somewhat skewed to the left (indicating older Tweets).  There are also some spikes during certain periods where many Tweets were made for a particular company, perhaps indicating some sort of event occurred.<br>\n",
    "\n",
    "For now, we have lumped the time-date stamp across the years all together for our analysis though we could consider bucketing the data by time blocks to see a flow of topics over time.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Time Series Statistics](slo-twitter-data-analysis-time-statistics.ipynb#bookmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retweet Statistics and Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "    We will experiment with different strategies, e.g., remove all retweets; use retweet counts to determine \"influencers\"; compute votes for/against original posts via stance analysis of retweet sets for individual tweets.\n",
    "    \n",
    "\n",
    "**TODO: remove after Professor VanderLinden gives the thumbs up for revised summary.**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Approximately 67% of our dataset are ReTweets while the other 33% aren't.  Most Tweets associated with \"Adani\" are ReTweets.  We see a more even distribution between ReTweets and non-ReTweets for the other companies.  Again, \"Adani\" Tweets heavily influence the distribution of our statistics and graphs.  Of the 446,177 ReTweets, we possess the orignal text of the ReTweeted Tweet for 445,533 of them.  We do not have the original text for 644 of them.  There are also a few orginal ReTweeted Tweets whose ReTweet counts are extremely high, with 98,886 being the highest.<br>\n",
    "\n",
    "In the future, we will experiment with different strategies, e.g., remove all retweets; use retweet counts to determine \"influencers\"; compute votes for/against original posts via stance analysis of retweet sets for individual tweets.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Retweet Statistics](slo-twitter-data-analysis-retweet-statistics.ipynb#bookmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User (Tweet Author) Statistics and Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "    What does this mean for our analysis plan? We could bucket tweets based on rule-based stance assignments (see autocoding-preprocessor.py). We could throw out neutral tweets. We could to a network analysis of authors to compute \"influencers\", \"new\" ideas. \n",
    "    \n",
    "We have far fewer unique authors HOW MANY? than we do total Tweets in the dataset.  From the stats, we see that the Twitter users that are actually (neutral) news outlet or organizations are responsible for the majority of Tweets and ReTweets.<br>\n",
    "\n",
    "**TODO: remove after Professor VanderLinden gives the thumbs up for revised summary.**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "At a unique author count of 38,107, we have far fewer unique authors than we do for the total of 670,423 Tweets in the dataset.  From the stats, we see that the Twitter users that are actually (neutral) news outlet or organizations are responsible for the majority of Tweets and ReTweets.  We also found that 375,104 of the 670,423 Tweet texts are over 140 characters long while only 256,373 user description texts are over 140 characters long.<br>\n",
    "\n",
    "What does this mean for our analysis plan? We could bucket tweets based on rule-based stance assignments (see autocoding-preprocessor.py). We could throw out neutral tweets. We could to a network analysis of authors to compute \"influencers\", \"new\" ideas.<br> \n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[User (Tweet Author) Statistics](slo-twitter-data-analysis-user-statistics.ipynb#bookmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Statistics and Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "    Combine all tweet text analysis here, including character counts and any other of Shutaro's analysis that seem useful.\n",
    "    We may need to move to Linux to use the CMU tweet tagger for this work.\n",
    "    \n",
    "\n",
    "**TODO: remove after Professor VanderLinden gives the thumbs up for revised summary.**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Most Tweets are under 300 characters long but many are over 140 characters long.  We surmise this is because our \"dataset_processor_adapted_v2.py\" dataset creator adds the full ReTweeted text to the \"text_derived\" field we derive for our dataset.  It is also possible there are encoding issues for foreign Tweets.  Further analysis is necessary.<br>\n",
    "\n",
    "**TODO: the rest of Shuntaro's analysis on Tweet text definitely utilizes CMU Tweet Tagger, which is undoable until the Linux workstations in the labs have been restored after the server corruption issue.**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tweet Text Statistics](slo-twitter-data-analysis-tweet-text-statistics.ipynb#bookmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #Hashtag Statistics and Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "40% of \"Adani\" Tweets do not have any hashtags while the rest have at least one hashtag.  According to Pandas, 367,220 Tweets have at least one hashtag, which comprise approximately 54.8% of our entire dataset.<br>\n",
    "\n",
    "**TODO: Attempt to get the two remaining text statistics sections of Shuntaro's code for this section functional.**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hashtags Statistics](slo-twitter-data-analysis-hashtag-statistics.ipynb#bookmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @User Mentions Statistics and Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "Approximately 79.85% of all Tweets in our dataset have @user mentions.  Only about 5.89% of all Tweets are replices to other Tweets in our dataset.  Most Tweets have at most a single user mention.<br>\n",
    "\n",
    "**Note: Do NOT use \"tweet_entities_user_mentions_id\" field for text statistics.  Twitter API uses empty Lists when there are no ID's, which is NOT the same as NaN/null so Pandas \".counts\" function will still count the Tweet as having user mentions.  Also, don't use for the 2nd graph/plot as the user ID fails to display properly adn the user mentions counts are not computing properly.**\n",
    "\n",
    "**TODO: Attempt to get 2nd graph/plot functioning properly.**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[User Mentions Statistics](slo-twitter-data-analysis-mentions-statistics.ipynb#bookmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Stock Symbols, URL's, and Emojis Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "Only approximately 10.3% of all the Tweets in our dataset possess stock symbols of some sort.  On the flip side, about 77.5% of all the Tweets in our dataset possess URL's.  As for emoji's, only approximately 0.34% of our Tweets have emoticons of some sort.\n",
    "\n",
    "**TODO: create separate Jupyter Notebook Files for each, if necessary.**<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stock Symbols, URL's, Emojis - Statistics](slo-twitter-data-analysis-stock-symbols-and-urls-emojis.ipynb#bookmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nan/Non-NaN Values Statistics and Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "This Jupyter Notebook file provides a overview on the number of null or non-null values for each attribute across the entire Twitter dataset.  It displays the count of how many rows (examples) in the dataset have NaN or non-NaN values for each column (field/attribute) that is present in our dataset.<br>\n",
    "\n",
    "**TODO: Replace this Jupyter Notebook File by refactoring into each individual category of analysis above and calling the Nan/Non-NaN data analysis function on the attributes covered in each section.**<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Nan/Non-NaN Statistics](slo-twitter-data-analysis-other-statistics.ipynb#bookmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "**TODO: convert to annotated bibliography**\n",
    "\n",
    "Dataset Files (obtained from Borg supercomputer):<br>\n",
    "\n",
    "dataset_slo_20100101-20180510.json<br>\n",
    "dataset_20100101-20180510.csv<br>\n",
    "\n",
    "Note: These are large files not included in the project GitHub Repository.<br>\n",
    "\n",
    "\n",
    "- [SLO-analysis.ipynb](SLO-analysis.ipynb)<br>\n",
    "    -original SLO Twitter data analysis file from Shuntaro Yada.<br>\n",
    "\n",
    "\n",
    "- https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/intro-to-tweet-json<br>\n",
    "    -explanation of all data fields in JSON file format for Tweets.<br>\n",
    "\n",
    "\n",
    "- https://datatofish.com/export-dataframe-to-csv/<br>\n",
    "- https://datatofish.com/export-pandas-dataframe-json/<br>\n",
    "    -saving Pandas dataframe to CSV/JSON<br>\n",
    "    \n",
    "\n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html<br>\n",
    "    -Pandas to_datetime() function call.<br>\n",
    "    \n",
    "\n",
    "- https://www.machinelearningplus.com/plots/matplotlib-tutorial-complete-guide-python-plot-examples/<br>\n",
    "    -plotting with matplotlib.<br>\n",
    "\n",
    "\n",
    "- https://stackoverflow.com/questions/49566007/jupyter-multiple-notebooks-using-same-data<br>\n",
    "     \n",
    "- https://stackoverflow.com/questions/16966280/reusing-code-from-different-ipython-notebooks<br>\n",
    "    -sharing kernels and code across multiple Jupyter notebook files<br>\n",
    "     \n",
    " \n",
    "- https://stackoverflow.com/questions/32370281/how-to-embed-image-or-picture-in-jupyter-notebook-either-from-a-local-machine-o\n",
    "    -displaying embedded images<br>\n",
    "    \n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
