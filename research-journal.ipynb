{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# Research Journal (SLO Topic Modeling)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Author: Joseph Jinn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## May 29, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.5em;\">\n",
    "\n",
    "Project Trello Board: https://trello.com/b/tXM2NIgT/slo-stance\n",
    "<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "Watched Calvin College Senior Project Presentations 2019 on SLO Topic Modeling and SLO Monitoring (stance analysis).\n",
    "\n",
    "Watched short promotional videos on Luminoso Daylight and Compass.\n",
    "\n",
    "Going through blogs and other tutorials on the basic concept behind LDA's.\n",
    "\n",
    "Implemented Scikit-Learn LDA example(s).\n",
    "  \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Your Easy Guide to Latent Dirichlet Allocation\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "    \n",
    "URL: https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n",
    "\n",
    "Alpha - controls the mixture of topics for any given document.\n",
    "\n",
    "Beta - the distribution of words per topic.\n",
    "\n",
    "Values are typically set below one.\n",
    "\n",
    "Composites - documents.\n",
    "\n",
    "Parts - individual items to be considered for each composite.\n",
    "\n",
    "Topics - categories that determine what each part belongs to.\n",
    "\n",
    "Choose # of topics < # of documents to reduce dimensionality for further analysis using other algorithms.\n",
    "\n",
    "Phi - the parts versus the topics matrix (or topics versus parts matrix)\n",
    "\n",
    "Theta - the composites versus topics matrix.\n",
    "\n",
    "Iterations - # of times to run the LDA.\n",
    "\n",
    "Start by randomly assigning a topic to each part.\n",
    "\n",
    "Use Gibbs sampling algorithm for the iteration.\n",
    "\n",
    "Generate Phi and Theta values/matrices.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Probabilistic Topic Models\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-family:Papyrus; font-size:1.25em;\">\n",
    "\n",
    "URL: https://cacm.acm.org/magazines/2012/4/147361-probabilistic-topic-models/fulltext\n",
    "\n",
    "1) Probababilistic topic modeling - suite of algorithms to discover and annotate large archives of documents with thematic information.\n",
    "\n",
    "2) topic - a distribution over a fixed vocabulary.\n",
    "\n",
    "3) for each document in collection, generate words by:\n",
    "\n",
    "    A) randomly choosing a distribution over topics.\n",
    "    B) for each word in the document:\n",
    "    \n",
    "        i) randomly choose a topic from the distribution over topics in step A.\n",
    "        ii) randomly choose a word from the corresponding distribution over the entire vocabulary.\n",
    "        \n",
    "4) Documents are the observed structure.\n",
    "\n",
    "5) Topic structure is the hidden structure.\n",
    "\n",
    "    A) topics.\n",
    "    B) per-document topic distributions.\n",
    "    C) per-document per-word topic assignments.\n",
    "    \n",
    "6) Goal is to find the hidden structure that generated the observed colection of documents.\n",
    "\n",
    "7) LDA probabilistic models part of probabilistic modeling.\n",
    "    \n",
    "    A) Generative process that includes hidden and observed random variables.\n",
    "    B) Use joint probability distribution to comput conditional distribution of hidden variables given observed variables.\n",
    "    C) Observed variables = words of the documents.\n",
    "    D) Hidden variables = topic structure.\n",
    "\n",
    "8) \n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## May 30, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "##### ida-preserntation.ipynb\n",
    "\n",
    "Tentatively created a Jupyter Notebook presentation on the basics of Latent Dirichlet Allocation from a statistical perspective.\n",
    "\n",
    "##### Immediate plans:\n",
    "\n",
    "Use GridSearchCV to see if I can automate hyper-parameter tuning to improve upon LDA topic extraction results.<br>\n",
    "\n",
    "Incorporate Derek Fisher's senior project codebase into my own code and adapt as necessary to reproduce and maybe improve upon his results.<br>\n",
    "\n",
    "Current codebase is contained in \"scikit-learn-lda-example.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
